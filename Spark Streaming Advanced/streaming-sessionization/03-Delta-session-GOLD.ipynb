{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f0326dc-9f97-43a6-a88a-1578e4892c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png)  3/ GOLD table: extract the sessions\n",
    "\n",
    "<img style=\"float:right; height: 250px; margin: 0px 30px 0px 30px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/session_diagram.png\">\n",
    "\n",
    "### Why is this a challenge?\n",
    "Because we don't have any event to flag the user disconnection, detecting the end of the session is hard. After 10 minutes without any events, we want to be notified that the session has ended.\n",
    "However, spark will only react on event, not the absence of event.\n",
    "\n",
    "Thanksfully, Spark Structured Streaming has the concept of timeout. \n",
    "\n",
    "**We can set a 10 minutes timeout in the state engine** and be notified 10 minutes later in order to close the session\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=3759185753378633&notebook=%2F03-Delta-session-GOLD&demo_name=streaming-sessionization&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fstreaming-sessionization%2F03-Delta-session-GOLD&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9542ad9-4e4e-4da2-bd1d-8d611094d55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-streaming-sessionization-maynard` from the dropdown menu ([open cluster configuration](https://adb-3759185753378633.13.azuredatabricks.net/#setting/clusters/0523-032130-3sdou7jm/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('streaming-sessionization')` or re-install the demo: `dbdemos.install('streaming-sessionization')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245f1754-de9a-4dd7-939f-465a84e38ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "169cb1ee-a766-4df9-bd38-2b190caa0f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Implementing the aggregation function to update our Session\n",
    "\n",
    "In this simple example, we'll just be counting the number of click in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d709627-85e3-4774-a730-ab12a3143961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterator\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "DBDemos.wait_for_table(\"events\") #Wait until the previous table is created to avoid error if all notebooks are started at once\n",
    "\n",
    "\n",
    "#If we don't have activity after 30sec, close the session\n",
    "max_session_duration = 30000\n",
    "\n",
    "def func( key: Tuple[str], events: Iterator[pd.DataFrame], state: GroupState ) -> Iterator[pd.DataFrame]:\n",
    "  \n",
    "  # Unpack the key tuple passed by Spark (even with a single group key like 'user_id', it's wrapped in a tuple)\n",
    "  (user_id,) = key\n",
    "  print(user_id)\n",
    "\n",
    "  if state.exists:\n",
    "    (user_id, click_count, start_time, end_time) = state.get\n",
    "  else:\n",
    "    click_count = 0\n",
    "    start_time = sys.maxsize\n",
    "    end_time = 0\n",
    "  # state.getOption\n",
    "\n",
    "  if state.hasTimedOut:\n",
    "    #Drop the session from the state and emit a final offline session update (end of the session)\n",
    "    state.remove()\n",
    "\n",
    "    yield pd.DataFrame({\"user_id\": [user_id], \"click_count\": [click_count], \"start_time\": [start_time], \"end_time\": [end_time],  \"status\": [\"offline\"]})\n",
    "\n",
    "  else:\n",
    "    # as we can receive out-of-order events, we need to get the min/max date and the sum\n",
    "    for df in events:\n",
    "      start_time = min(start_time, df['event_date'].min())\n",
    "      end_time = max(df['event_date'].max(), end_time)\n",
    "      click_count += len(df)\n",
    "\n",
    "    #update the state with the new values\n",
    "    state.update((user_id, int(click_count), int(start_time), int(end_time)))\n",
    "\n",
    "    # Set the timeout as max_session_duration seconds.\n",
    "    state.setTimeoutDuration(max_session_duration)\n",
    "\n",
    "    #compute the status to flag offline session in case of restart\n",
    "    now = int(time.time())\n",
    "    status = \"offline\" if end_time >= now - max_session_duration else \"online\"\n",
    "    \n",
    "    #emit the change. We could also yield an empty dataframe if we only want to emit when the session is closed: yield pd.DataFrame()\n",
    "    yield pd.DataFrame({\"user_id\": [user_id], \"click_count\": [click_count], \"start_time\": [start_time], \"end_time\": [end_time],  \"status\": [status]})\n",
    "\n",
    "\n",
    "output_schema = \"user_id STRING, click_count LONG, start_time LONG, end_time LONG, status STRING\"\n",
    "state_schema = \"user_id STRING, click_count LONG, start_time LONG, end_time LONG\"\n",
    "\n",
    "# Enable processing-time-based timeouts for each group.\n",
    "# This allows state.hasTimedOut and state.setTimeoutDuration(...) to work,\n",
    "# so we can close sessions after a period of inactivity (e.g., 30 seconds).\n",
    "sessions = spark.readStream.table(\"events\").groupBy(F.col(\"user_id\")).applyInPandasWithState(\n",
    "    func,\n",
    "    output_schema,\n",
    "    state_schema,\n",
    "    \"append\",\n",
    "    GroupStateTimeout.ProcessingTimeTimeout)\n",
    "\n",
    "display(sessions, checkpointLocation = get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ca9a64-f7d6-4659-929d-d25ca2ccab19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Updating the session table with number of clicks and end/start time\n",
    "\n",
    "We want to have the session information in real time for each user. \n",
    "\n",
    "To do that, we'll create a Session table. Everytime we update the state, we'll UPSERT the session information:\n",
    "\n",
    "- if the session doesn't exist, we add it\n",
    "- if it exists, we update it with the new count and potential new status\n",
    "\n",
    "This can easily be done with a MERGE operation using Delta and calling `foreachBatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b59886-762e-43f1-b31c-ad273e8a32f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_sessions(df, epoch_id):\n",
    "  #Create the table if it's the first time (we need it to be able to perform the merge)\n",
    "  #limit(0) to create a DataFrame with the correct schema but zero rows.\n",
    "  #spark._jsparkSession gives you access to the underlying Java/Scala SparkSession (used in internal checks like tableExists).\n",
    "  if epoch_id == 0 and not spark._jsparkSession.catalog().tableExists('sessions'):\n",
    "    df.limit(0).write.option('mergeSchema', 'true').mode('append').saveAsTable('sessions')\n",
    "    \n",
    "  #Load Delta table by name (registered in metastore) for Delta-specific methods\n",
    "  (DeltaTable.forName(spark, \"sessions\").alias(\"s\").merge(\n",
    "    source = df.alias(\"u\"),\n",
    "    condition = \"s.user_id = u.user_id\")\n",
    "  .whenMatchedUpdateAll()\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute())\n",
    "  \n",
    "(sessions.writeStream\n",
    "  .option(\"checkpointLocation\", volume_folder+\"/checkpoints/sessions\")\n",
    "  .foreachBatch(upsert_sessions)\n",
    "  .start())\n",
    "\n",
    "DBDemos.wait_for_table(\"sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2284e180-f822-4757-81b8-f585360f9f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691aadb6-4847-4f02-a093-9800d5a903c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT CAST(avg(end_time - start_time) as INT) average_session_duration FROM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef896f2-34f2-4d49-b04e-bed7b6d3512d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stop all the streams "
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams(sleep_time=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aecfce71-edcd-4f22-81ab-4e78af037de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We now have our sessions stream running!\n",
    "\n",
    "We can set the output of this streaming job to a SQL database or another queuing system.\n",
    "\n",
    "We'll be able to automatically detect cart abandonments in our website and send an email to our customers, our maybe just give them a call asking if they need some help! "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4400352886973699,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03-Delta-session-GOLD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
