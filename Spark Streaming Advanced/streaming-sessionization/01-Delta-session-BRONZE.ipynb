{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8450b2ba-1028-4838-b50a-daa27306e575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Streaming on Databricks with Spark and Delta Lake\n",
    "\n",
    "Streaming on Databricks is greatly simplified using Delta Live Table (DLT). <br/>\n",
    "DLT lets you write your entire data pipeline, supporting streaming transformation using SQL or python and removing all the technical challenges.\n",
    "\n",
    "We strongly recommend implementing your pipelines using DLT as this will allow for much robust pipelines, enforcing data quality and greatly accelerating project delivery.<br/>\n",
    "*For a DLT example, please install `dbdemos.install('dlt-loans')` or the C360 Lakehouse demo: `dbdemos.install('lakehouse-retail-churn')`*\n",
    "\n",
    "Spark Streaming API offers lower-level primitive offering more advanced control, such as `foreachBatch` and custom streaming operation with `applyInPandasWithState`.\n",
    "\n",
    "Some advanced use-case can be implemented using these APIs, and this is what we'll focus on.\n",
    "\n",
    "## Building a sessionization stream with Delta Lake and Spark Streaming\n",
    "\n",
    "### What's sessionization?\n",
    "<div style=\"float:right\" ><img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/session_diagram.png\" style=\"height: 200px; margin:0px 0px 0px 10px\"/></div>\n",
    "\n",
    "Sessionization is the process of finding time-bounded user session from a flow of event, grouping all events happening around the same time (ex: number of clicks, pages most view etc)\n",
    "\n",
    "When there is a temporal gap greater than X minute, we decide to split the session in 2 distinct sessions\n",
    "\n",
    "### Why is that important?\n",
    "\n",
    "Understanding sessions is critical for a lot of use cases:\n",
    "\n",
    "- Detect cart abandonment in your online shot, and automatically trigger marketing actions as follow-up to increase your sales\n",
    "- Build better attribution model for your affiliation, based on the user actions during each session \n",
    "- Understand user journey in your website, and provide better experience to increase your user retention\n",
    "- ...\n",
    "\n",
    "\n",
    "### Sessionization with Spark & Delta\n",
    "\n",
    "Sessionization can be done in many ways. SQL windowing is often used but quickly become too restricted for complex use-case. \n",
    "\n",
    "Instead, we'll be using the following Delta Architecture:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/sessionization.png\" width=\"1200px\">\n",
    "\n",
    "Being able to process and aggregate your sessions in a Batch and Streaming fashion can be a real challenge, especially when updates are required in your historical data!\n",
    "\n",
    "Thankfully, Delta and Spark can simplify our job, using Spark Streaming function with a custom stateful operation (`flatMapGroupsWithState` operator), in a streaming and batch fashion.\n",
    "\n",
    "Let's build our Session job to detect cart abandonment !\n",
    "\n",
    "\n",
    "*Note: again, this is an advanced demo - if you're starting with Databricks and are looking for a simple streaming pipeline we recommand going with DLT instead.*\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=3759185753378633&notebook=%2F01-Delta-session-BRONZE&demo_name=streaming-sessionization&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fstreaming-sessionization%2F01-Delta-session-BRONZE&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88cd1c15-f6fa-4400-b65f-2006d25ca2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-streaming-sessionization-maynard` from the dropdown menu ([open cluster configuration](https://adb-3759185753378633.13.azuredatabricks.net/#setting/clusters/0523-032130-3sdou7jm/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('streaming-sessionization')` or re-install the demo: `dbdemos.install('streaming-sessionization')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7524bd0-aa6d-458d-82b2-369004c0d9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## First, make sure events are published to your kafka queue\n",
    "\n",
    "Start the [_00-Delta-session-PRODUCER]($./_00-Delta-session-PRODUCER) notebook to send messages to your kafka queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e734f2-0a67-4be8-9549-20b1155948a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f36ab799-9f40-4515-bc75-226bedb7f526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 1/ Bronze table: store the stream as Delta Lake table\n",
    "\n",
    "<img style=\"float:right; height: 250px; margin: 0px 30px 0px 30px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/sessionization_bronze.png\">\n",
    "\n",
    "The first step is to consume data from our streaming engine (Kafka, Kinesis, Pulsar etc.) and save it in our Data Lake.\n",
    "\n",
    "We won't be doing any transformation, the goal is to be able to re-process all the data and change/improve the downstream logic when needed\n",
    "\n",
    "#### Solving small files and compaction issues\n",
    "\n",
    "Everytime we capture kafka events, they'll be stored in our table and this will create new files. After several days, we'll endup with millions of small files leading to performance issues.<br/>\n",
    "Databricks solves that with autoOptimize & autoCompact, 2 properties to set at the table level.\n",
    "\n",
    "*Note that if the table isn't created with all the columns. The engine will automatically add the new column from kafka at write time, merging the schema gracefuly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3751c7d8-8ac1-4377-b27a-41b515aea8bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the table events_raw"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS events_raw (key string, value string) \n",
    "TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, \n",
    "               delta.autoOptimize.autoCompact = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c297b59e-5006-4c0f-983e-58126270a09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(volume_folder+\"/checkpoints\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd5913f-b251-4ca7-b5db-80ea6c145bb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read messages from Kafka and save them as events_raw"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: the demo runs with Kafka, and dbdemos doesn't publically expose its demo kafka servers. Use your own IPs to run the demo properly\n",
    "kafka_bootstrap_servers_tls = \"<Replace by your own kafka servers>\"\n",
    "# Also make sure to have the proper instance profile to allow the access if you're on AWS.\n",
    "\n",
    "stream = (spark\n",
    "    .readStream\n",
    "       #=== Configurations for Kafka streams ===\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers_tls) \n",
    "      .option(\"kafka.security.protocol\", \"SASL_SSL\") #SSL\n",
    "      .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "      .option(\"kafka.sasl.jaas.config\", \n",
    "        '<your-connection>')\n",
    "      .option(\"subscribe\", \"dbdemos-sessions\") #kafka topic\n",
    "      .option(\"startingOffsets\", \"latest\") #Consume messages from the end\n",
    "      .option(\"maxOffsetsPerTrigger\", \"10000\") # Control ingestion rate - backpressure\n",
    "      #.option(\"ignoreChanges\", \"true\")\n",
    "    .load()\n",
    "    .withColumn('key', col('key').cast('string'))\n",
    "    .withColumn('value', col('value').cast('string'))\n",
    "    .writeStream\n",
    "       # === Write to the delta table ===\n",
    "      .format(\"delta\")\n",
    "      .trigger(processingTime=\"20 seconds\")\n",
    "      #.trigger(availableNow=True) --use this for serverless\n",
    "      .option(\"checkpointLocation\", volume_folder+\"/checkpoints/bronze\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"events_raw\"))\n",
    "\n",
    "DBDemos.wait_for_table(\"events_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0e7962-15a4-4c4f-a485-ec5814b175af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test = (spark\n",
    "    .read\n",
    "       #=== Configurations for Kafka streams ===\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", \"<your-sever>\") \n",
    "      .option(\"kafka.security.protocol\", \"SASL_SSL\") #SSL\n",
    "      .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "      .option(\"kafka.sasl.jaas.config\", \n",
    "        '<your-connection>')\n",
    "      .option(\"subscribe\", \"dbdemos-sessions\") #kafka topic\n",
    "      .option(\"startingOffsets\", \"earliest\") #Consume messages from the end\n",
    "      .option(\"maxOffsetsPerTrigger\", \"10000\") # Control ingestion rate - backpressure\n",
    "      #.option(\"ignoreChanges\", \"true\")\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0946c9-8144-4b89-a864-b2b0bc1ac360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b40f30b-5b63-42a2-9f90-980935e1c908",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Our table events_raw is ready and will contain all events"
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM events_raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90bb9b9d-db05-45aa-a2ea-c0dbb6df86bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Our Raw events are now ready to be analyzed\n",
    "\n",
    "It's now easy to run queries in our events_raw table. Our data is saved as JSON, databricks makes it easy to query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d38baf4-c66d-46f5-8e0b-3e2842b84106",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Action per platform"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*), \n",
    "       value:platform as platform \n",
    "from events_raw \n",
    "group by platform;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429ca4a2-88e3-4bc2-b58b-43cd9b6882f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Searching for duplicate events\n",
    "\n",
    "As you can see, our producer sends incorrect messages.\n",
    "\n",
    "Not only we have null event_id from time to time, but we also have duplicate events (identical events being send twice with the same ID and exact same content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755f1a7b-78b1-4a39-8813-1cecc4039862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) event_count, \n",
    "       value:event_id event_id, \n",
    "       first(value) from events_raw\n",
    "group by event_id\n",
    "having event_count > 1\n",
    "order by event_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fdd254b-1ca4-487e-9920-0a32388ed4b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stop all the streams "
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams(sleep_time=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa0dec3-6bf0-400a-95ef-877d7df50c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps: Cleanup data and remove duplicates\n",
    "\n",
    "It looks like we have duplicate event in our dataset. Let's see how we can perform some cleanup. \n",
    "\n",
    "In addition, reading from JSON isn't super efficient, and what if our json changes over time ?\n",
    "\n",
    "While we can explore the dataset using spark json manipulation, this isn't ideal. For example is the json in our message changes after a few month, our request will fail.\n",
    "\n",
    "Futhermore, performances won't be great at scale: because all our data is stored as a unique, we can't leverage data skipping and a columnar format\n",
    "\n",
    "That's why we need another table:  **[A Silver Table!]($./02-Delta-session-SILVER)**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4400352886973655,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01-Delta-session-BRONZE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
