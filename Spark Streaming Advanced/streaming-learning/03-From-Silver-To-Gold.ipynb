{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b386aec3-a493-4e38-a369-232c092b5602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/01-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6bad11-e54d-4c07-a825-cbf699095b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The aggregation function to update session via applyInPandasWithState():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56bbb40b-0529-4a42-9563-311b73f7e2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterator\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "# If we don't have activity after 30sec, close the session:\n",
    "max_session_duration = 30000\n",
    "\n",
    "def func(key: Tuple[str], events: Iterator[pd.DataFrame], state: GroupState) -> Iterator[pd.DataFrame]:\n",
    "    # Unpack the key tuple passed by Spark.\n",
    "    # Even with a single group key like 'user_id', it's wrapped in a tuple.\n",
    "    (user_id,) = key\n",
    "\n",
    "    # Get current session's data if state exists.\n",
    "    # If not, set default data for a new session.\n",
    "    if state.exists:\n",
    "        (user_id, click_count, start_time, end_time) = state.get\n",
    "    else:\n",
    "        click_count = 0\n",
    "        start_time = sys.maxsize\n",
    "        end_time = 0\n",
    "    \n",
    "    if state.hasTimedOut:\n",
    "        # End of the session: Drop the session from the state.\n",
    "        state.remove()\n",
    "        # Emit a final offline session update.\n",
    "        yield pd.DataFrame({\"user_id\": [user_id], \"click_count\": [click_count],\n",
    "                            \"start_time\": [start_time], \"end_time\": [end_time], \"status\": [\"offline\"]})\n",
    "    else:\n",
    "        # For out-of-order events, we need to get the min/max date and the sum:\n",
    "        for df in events:\n",
    "            start_time = min(start_time, df[\"event_date\"].min())\n",
    "            end_time = max(end_time, df[\"event_date\"].max())\n",
    "            click_count += len(df)\n",
    "\n",
    "        # Update the state with the new values:\n",
    "        state.update((user_id, int(click_count), int(start_time), int(end_time)))\n",
    "\n",
    "        # Set the timeout as max_session_duration seconds:\n",
    "        state.setTimeoutDuration(max_session_duration)\n",
    "\n",
    "        # Compute the status to flag offline session in case of restart:\n",
    "        now = int(time.time())\n",
    "        status = \"offline\" if end_time >= now - max_session_duration else \"online\"\n",
    "    \n",
    "        # Emit the change. \n",
    "        # We could also yield an empty dataframe if we only want to emit when the session is closed: yield pd.DataFrame()\n",
    "        yield pd.DataFrame({\"user_id\": [user_id], \"click_count\": [click_count],\n",
    "                            \"start_time\": [start_time], \"end_time\": [end_time], \"status\": [status]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70746983-e1f4-4a51-8d20-514b923b8095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_schema = \"user_id STRING, click_count LONG, start_time LONG, end_time LONG, status STRING\"\n",
    "state_schema = \"user_id STRING, click_count LONG, start_time LONG, end_time LONG\"\n",
    "\n",
    "# Enable processing-time-based timeouts for each group.\n",
    "# This allows state.hasTimedOut and state.setTimeoutDuration(...) to work,\n",
    "# so we can close sessions after a period of inactivity (e.g., 30 seconds).\n",
    "sessions = (spark.readStream.table(\"events\")\n",
    "            .groupBy(F.col(\"user_id\"))\n",
    "            .applyInPandasWithState(\n",
    "                func,\n",
    "                output_schema,\n",
    "                state_schema,\n",
    "                \"append\",\n",
    "                GroupStateTimeout.ProcessingTimeTimeout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63e6d9de-d604-42f3-a43e-ff57bd28af06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sessions, get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1689436-4cc7-4d8b-bdc3-3105a7e446cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Updating the session table with number of clicks and end/start time:\n",
    "\n",
    "We want to have the session information in real time for each user. \n",
    "\n",
    "To do that, we'll create a Session table. Everytime we update the state, we'll UPSERT the session information with a MERGE operation using Delta and calling `foreachBatch`:\n",
    "\n",
    "- If the session doesn't exist, we add it.\n",
    "- If it exists, we update it with the new count and potential new status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92593ee8-fb8c-4407-9af3-79a2b0056a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_sessions(df, epoch_id):\n",
    "    #Create the table if it's the first time (we need it to be able to perform the merge)\n",
    "    #limit(0) to create a DataFrame with the correct schema but zero rows.\n",
    "    #spark._jsparkSession gives you access to the underlying Java/Scala SparkSession (used in internal checks like tableExists).\n",
    "    if epoch_id == 0 and not spark._jsparkSession.catalog().tableExists(\"sessions\"):\n",
    "        (df.limit(0).write\n",
    "                        .option('mergeSchema', 'true')\n",
    "                        .mode('append')\n",
    "                        .saveAsTable(\"sessions\"))\n",
    "\n",
    "    #Load Delta table by name (registered in metastore) for Delta-specific methods\n",
    "    (DeltaTable.forName(spark, \"sessions\").alias(\"s\")\n",
    "                        .merge(\n",
    "                            source = df.alias(\"u\"),\n",
    "                            condition = \"s.user_id = u.user_id\")\n",
    "                        .whenMatchedUpdateAll()\n",
    "                        .whenNotMatchedInsertAll()\n",
    "                        .execute())\n",
    "    \n",
    "(sessions.writeStream\n",
    " .option(\"checkpointLocation\", volume_folder + \"/checkpoints/gold\")\n",
    " .foreachBatch(upsert_sessions)\n",
    " .start())\n",
    "\n",
    "Utils.wait_for_table(\"sessions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0259fa-e937-4856-a54f-44e024414834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sessions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5929a3d-5788-4fe9-a50e-6ec7b30fdfd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT CAST(AVG(end_time - start_time) AS INT) average_session_duration \n",
    "FROM sessions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69f7a0f1-7ad3-4bf6-8876-1cf3f11fcf45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Utils.stop_all_streams(sleep_time=120)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6057896943036757,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-From-Silver-To-Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
