{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6835c15-693e-49dc-8eea-5a827f671a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Technical Setup notebook. Hide this cell results\n",
    "Initialize dataset to the current user and cleanup data when reset_all_data is set to true\n",
    "\n",
    "Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0524370b-d9a2-47a2-b3a3-688579e69854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.text(\"min_dbr_version\", \"12.2\", \"Min required DBR version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3535b8b2-734f-4c86-9628-08bce8b7267a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "class DBDemos():\n",
    "  @staticmethod\n",
    "  def setup_schema(catalog, db, reset_all_data, volume_name = None):\n",
    "    if reset_all_data:\n",
    "      print(f'clearing up volume named `{catalog}`.`{db}`.`{volume_name}`')\n",
    "      try:\n",
    "        spark.sql(f\"DROP VOLUME IF EXISTS `{catalog}`.`{db}`.`{volume_name}`\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS `{catalog}`.`{db}` CASCADE\")\n",
    "      except Exception as e:\n",
    "        print(f'catalog `{catalog}` or schema `{db}` do not exist.  Skipping data reset')\n",
    "\n",
    "    def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "      print(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "    assert catalog not in ['hive_metastore', 'spark_catalog'], \"This demo only support Unity. Please change your catalog name.\"\n",
    "    #If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "    current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "    if current_catalog != catalog:\n",
    "      catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "      if catalog not in catalogs:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}` MANAGED LOCATION 'abfss://unity-catalog-storage@dbstoragefuavvebhyzyxm.dfs.core.windows.net/3759185753378633'\")\n",
    "        if catalog == 'dbdemos':\n",
    "          spark.sql(f\"ALTER CATALOG `{catalog}` OWNER TO `account users`\")\n",
    "    use_and_create_db(catalog, db)\n",
    "\n",
    "    if catalog == 'dbdemos':\n",
    "      try:\n",
    "        spark.sql(f\"GRANT CREATE, USAGE on DATABASE `{catalog}`.`{db}` TO `account users`\")\n",
    "        spark.sql(f\"ALTER SCHEMA `{catalog}`.`{db}` OWNER TO `account users`\")\n",
    "        for t in spark.sql(f'SHOW TABLES in {catalog}.{db}').collect():\n",
    "          try:\n",
    "            spark.sql(f'GRANT ALL PRIVILEGES ON TABLE {catalog}.{db}.{t[\"tableName\"]} TO `account users`')\n",
    "            spark.sql(f'ALTER TABLE {catalog}.{db}.{t[\"tableName\"]} OWNER TO `account users`')\n",
    "          except Exception as e:\n",
    "            if \"NOT_IMPLEMENTED.TRANSFER_MATERIALIZED_VIEW_OWNERSHIP\" not in str(e) and \"STREAMING_TABLE_OPERATION_NOT_ALLOWED.UNSUPPORTED_OPERATION\" not in str(e) :\n",
    "              print(f'WARN: Couldn t set table {catalog}.{db}.{t[\"tableName\"]} owner to account users, error: {e}')\n",
    "      except Exception as e:\n",
    "        print(\"Couldn't grant access to the schema to all users:\"+str(e))    \n",
    "\n",
    "    print(f\"using catalog.database `{catalog}`.`{db}`\")\n",
    "    spark.sql(f\"\"\"USE `{catalog}`.`{db}`\"\"\")    \n",
    "\n",
    "    if volume_name:\n",
    "      spark.sql(f'CREATE VOLUME IF NOT EXISTS {volume_name};')\n",
    "\n",
    "                     \n",
    "  #Return true if the folder is empty or does not exists\n",
    "  @staticmethod\n",
    "  def is_folder_empty(folder):\n",
    "    try:\n",
    "      return len(dbutils.fs.ls(folder)) == 0\n",
    "    except:\n",
    "      return True\n",
    "    \n",
    "  @staticmethod\n",
    "  def is_any_folder_empty(folders):\n",
    "    return any([DBDemos.is_folder_empty(f) for f in folders])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_model_permission(model_name, permission, principal):\n",
    "    import databricks.sdk.service.catalog as c\n",
    "    sdk_client = databricks.sdk.WorkspaceClient()\n",
    "    return sdk_client.grants.update(c.SecurableType.FUNCTION, model_name, changes=[\n",
    "                              c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_model_endpoint_permission(endpoint_name, permission, group_name):\n",
    "    import databricks.sdk.service.serving as s\n",
    "    sdk_client = databricks.sdk.WorkspaceClient()\n",
    "    ep = sdk_client.serving_endpoints.get(endpoint_name)\n",
    "    return sdk_client.serving_endpoints.set_permissions(serving_endpoint_id=ep.id, access_control_list=[s.ServingEndpointAccessControlRequest(permission_level=s.ServingEndpointPermissionLevel[permission], group_name=group_name)])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_index_permission(index_name, permission, principal):\n",
    "      import databricks.sdk.service.catalog as c\n",
    "      sdk_client = databricks.sdk.WorkspaceClient()\n",
    "      return sdk_client.grants.update(c.SecurableType.TABLE, index_name, changes=[\n",
    "                              c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "    \n",
    "\n",
    "  @staticmethod\n",
    "  def download_file_from_git(dest, owner, repo, path):\n",
    "    def download_file(url, destination):\n",
    "      local_filename = url.split('/')[-1]\n",
    "      # NOTE the stream=True parameter below\n",
    "      with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192): \n",
    "            # If you have chunk encoded response uncomment if\n",
    "            # and set chunk_size parameter to None.\n",
    "            #if chunk: \n",
    "            f.write(chunk)\n",
    "      return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "      try:\n",
    "        #Temporary fix to avoid hitting github limits - Swap github to our S3 bucket to download files\n",
    "        s3url = url.replace(\"https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/\", \"https://dbdemos-dataset.s3.amazonaws.com/\")\n",
    "        download_file(s3url, dest)\n",
    "      except:\n",
    "        download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "      collections.deque(executor.map(download_to_dest, files))\n",
    "         \n",
    "\n",
    "  #force the experiment to the field demos one. Required to launch as a batch\n",
    "  @staticmethod\n",
    "  def init_experiment_for_batch(demo_name, experiment_name):\n",
    "    import mlflow\n",
    "    #You can programatically get a PAT token with the following\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    xp_root_path = f\"/Shared/dbdemos/experiments/{demo_name}\"\n",
    "    try:\n",
    "      r = w.workspace.mkdirs(path=xp_root_path)\n",
    "    except Exception as e:\n",
    "      print(f\"ERROR: couldn't create a folder for the experiment under {xp_root_path} - please create the folder manually or  skip this init (used for job only: {e})\")\n",
    "      raise e\n",
    "    xp = f\"{xp_root_path}/{experiment_name}\"\n",
    "    print(f\"Using common experiment under {xp}\")\n",
    "    mlflow.set_experiment(xp)\n",
    "    DBDemos.set_experiment_permission(xp)\n",
    "    return mlflow.get_experiment_by_name(xp)\n",
    "\n",
    "  @staticmethod\n",
    "  def set_experiment_permission(experiment_path):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service import iam\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "      status = w.workspace.get_status(experiment_path)\n",
    "      w.permissions.set(\"experiments\", request_object_id=status.object_id,  access_control_list=[\n",
    "                            iam.AccessControlRequest(group_name=\"users\", permission_level=iam.PermissionLevel.CAN_MANAGE)])    \n",
    "    except Exception as e:\n",
    "      print(f\"error setting up shared experiment {experiment_path} permission: {e}\")\n",
    "\n",
    "    print(f\"Experiment on {experiment_path} was set public\")\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams_asynch(start_with = \"\", sleep_time=0):\n",
    "    import threading\n",
    "    def stop_streams():\n",
    "        DBDemos.stop_all_streams(start_with=start_with, sleep_time=sleep_time)\n",
    "\n",
    "    thread = threading.Thread(target=stop_streams)\n",
    "    thread.start()\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "    import time\n",
    "    time.sleep(sleep_time)\n",
    "    streams = DBDemos.get_active_streams(start_with)\n",
    "    if len(streams) > 0:\n",
    "      print(f\"Stopping {len(streams)} streams\")\n",
    "      for s in streams:\n",
    "          try:\n",
    "              s.stop()\n",
    "          except:\n",
    "              pass\n",
    "      print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "\n",
    "  @staticmethod\n",
    "  def wait_for_all_stream(start = \"\"):\n",
    "    import time\n",
    "    actives = DBDemos.get_active_streams(start)\n",
    "    if len(actives) > 0:\n",
    "      print(f\"{len(actives)} streams still active, waiting... ({[s.name for s in actives]})\")\n",
    "    while len(actives) > 0:\n",
    "      spark.streams.awaitAnyTermination()\n",
    "      time.sleep(1)\n",
    "      actives = DBDemos.get_active_streams(start)\n",
    "    print(\"All streams completed.\")\n",
    "\n",
    "  @staticmethod\n",
    "  def get_last_experiment(demo_name, experiment_path = \"/Shared/dbdemos/experiments/\"):\n",
    "    import requests\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    #TODO: waiting for https://github.com/databricks/databricks-sdk-py/issues/509 to use the python sdk instead\n",
    "    base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    r = requests.get(base_url+\"/api/2.0/workspace/list\", params={'path': f\"{experiment_path}/{demo_name}\"}, headers=headers).json()\n",
    "    if 'objects' not in r:\n",
    "      raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "    xps = [f for f in r['objects'] if f['object_type'] == 'MLFLOW_EXPERIMENT' and 'automl' in f['path']]\n",
    "    xps = [x for x in xps if re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', x['path'])]\n",
    "    sorted_xp = sorted(xps, key=lambda f: f['path'], reverse = True)\n",
    "    if len(sorted_xp) == 0:\n",
    "      raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "\n",
    "    last_xp = sorted_xp[0]\n",
    "\n",
    "    # Search for the date pattern in the input string\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', last_xp['path'])\n",
    "\n",
    "    if match:\n",
    "        date_str = match.group(1)  # Extract the matched date string\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d_%H:%M:%S')  # Convert to a datetime object\n",
    "        # Calculate the difference in days from the current date\n",
    "        days_difference = (datetime.now() - date).days\n",
    "        if days_difference > 30:\n",
    "            raise Exception(f\"It looks like the last experiment {last_xp} is too old ({days_difference} days). Please re-run the previous notebook to make sure you have the latest version. Delete the experiment folder if needed to clear history.\")\n",
    "    else:\n",
    "        raise Exception(f\"Invalid experiment format or no experiment available. Please re-run the previous notebook. {last_xp['path']}\")\n",
    "    return last_xp\n",
    "  \n",
    "\n",
    "  @staticmethod\n",
    "  def wait_for_table(table_name, timeout_duration=120):\n",
    "    import time\n",
    "    i = 0\n",
    "    while not spark.catalog.tableExists(table_name) or spark.table(table_name).count() == 0:\n",
    "      time.sleep(1)\n",
    "      if i > timeout_duration:\n",
    "        raise Exception(f\"couldn't find table {table_name} or table is empty. Do you have data being generated to be consumed?\")\n",
    "      i += 1\n",
    "\n",
    "\n",
    "  # Workaround for dbdemos to support automl the time being, creates a mock run simulating automl results\n",
    "  @staticmethod\n",
    "  def create_mockup_automl_run(full_xp_path, df, model_name=None, target_col=None):\n",
    "    import mlflow\n",
    "    import os\n",
    "    print(\"AutoML doesn't seem to be available, creating a mockup automl run instead - automl serverless will be added soon...\")\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    # Initialize the WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    w.workspace.mkdirs(path=os.path.dirname(full_xp_path))\n",
    "    xp = mlflow.create_experiment(full_xp_path)\n",
    "    mlflow.set_experiment(experiment_id=xp)\n",
    "    with mlflow.start_run(run_name=\"DBDemos automl mock autoML run\", experiment_id=xp) as run:\n",
    "        mlflow.set_tag('mlflow.source.name', 'Notebook: DataExploration')\n",
    "        mlflow.log_metric('val_f1_score', 0.81)\n",
    "        \n",
    "        split_choices = ['train', 'val', 'test']\n",
    "        split_probabilities = [0.7, 0.2, 0.1]  # 70% train, 20% val, 10% test\n",
    "        # Add a new column with random assignments\n",
    "        import numpy as np\n",
    "        df['_automl_split_col'] = np.random.choice(split_choices, size=len(df), p=split_probabilities)\n",
    "        import uuid\n",
    "        import os\n",
    "        random_path = f\"/tmp/{uuid.uuid4().hex}/dataset.parquet\"\n",
    "        os.makedirs(os.path.dirname(random_path), exist_ok=True)\n",
    "        df.to_parquet(random_path, index=False)\n",
    "        mlflow.log_artifact(random_path, artifact_path='data/training_data')\n",
    "        model = None\n",
    "        if model_name is not None and target_col is not None:\n",
    "          from sklearn.ensemble import RandomForestClassifier\n",
    "          from sklearn.metrics import f1_score\n",
    "          import pandas as pd\n",
    "\n",
    "          class SafeRandomForestClassifier(RandomForestClassifier):\n",
    "              def fit(self, X, y=None, sample_weight=None):\n",
    "                  # Auto-drop datetime columns\n",
    "                  if isinstance(X, pd.DataFrame):\n",
    "                      datetime_cols = X.select_dtypes(include=['datetime']).columns\n",
    "                      if len(datetime_cols) > 0:\n",
    "                          X = X.drop(columns=datetime_cols)\n",
    "                  return super().fit(X, y, sample_weight)\n",
    "\n",
    "              def predict(self, X):\n",
    "                  # Same: drop datetime columns at predict time\n",
    "                  if isinstance(X, pd.DataFrame):\n",
    "                      datetime_cols = X.select_dtypes(include=['datetime']).columns\n",
    "                      if len(datetime_cols) > 0:\n",
    "                          X = X.drop(columns=datetime_cols)\n",
    "                  return super().predict(X)\n",
    "                \n",
    "          # Split the data based on _automl_split_col\n",
    "          train_df = df[df['_automl_split_col'] == 'train']\n",
    "          val_df = df[df['_automl_split_col'] == 'val']\n",
    "          \n",
    "          # Prepare training and validation datasets\n",
    "          X_train = train_df.drop(columns=['_automl_split_col', target_col], errors='ignore')\n",
    "          y_train = train_df[target_col]\n",
    "          X_val = val_df.drop(columns=['_automl_split_col', target_col], errors='ignore')\n",
    "          y_val = val_df[target_col]\n",
    "          \n",
    "          # Train RandomForest model\n",
    "          model = SafeRandomForestClassifier(random_state=42)\n",
    "          model.fit(X_train, y_train)\n",
    "          y_pred = model.predict(X_val)\n",
    "          val_f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "          \n",
    "          # Log model and metric to MLflow\n",
    "          mlflow.log_metric('val_f1_score', val_f1)\n",
    "          mlflow.sklearn.log_model(model, artifact_path=\"model\", input_example=X_train.iloc[[0]])\n",
    "\n",
    "        class BestTrialMock:\n",
    "            def __init__(self, mlflow_run_id, model):\n",
    "                self.mlflow_run_id = mlflow_run_id\n",
    "                self.model = model\n",
    "                run_data = mlflow.get_run(mlflow_run_id).data\n",
    "                self.metrics = run_data.metrics\n",
    "                self.params = run_data.params\n",
    "            def load_model(self):\n",
    "                return self.model\n",
    "        \n",
    "        class XPMock:\n",
    "            def __init__(self, experiment_id):\n",
    "                self.experiment_id = experiment_id\n",
    "        \n",
    "        class AutoMLRun:\n",
    "            def __init__(self, best_trial_mlflow_run_id, model, xp):\n",
    "                self.best_trial = BestTrialMock(best_trial_mlflow_run_id, model)\n",
    "                self.experiment = XPMock(xp)\n",
    "        \n",
    "        return AutoMLRun(run.info.run_id, model, xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cec211a1-2b89-4e7c-8b89-daf39fb42b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's skip some warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-global-setup-v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
