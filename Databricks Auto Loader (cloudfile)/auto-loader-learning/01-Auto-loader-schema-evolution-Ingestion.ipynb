{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92127cf7-f99d-4ab5-ae0a-5c9d302aa342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is Databricks Auto Loader?\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader/autoloader-edited-anim.gif\" style=\"float:right; margin-left: 10px\" />\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "Auto Loader provides a strong gaurantee when used with a Delta sink (the data will only be ingested once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eceaa6c6-93f4-46ae-913e-6f61301caa1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How Auto Loader simplifies data ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard at scale. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "  * Using incremental listing (recommended, relies on filename order)\n",
    "  * Leveraging notification + message queue (when incremental listing can't be used)\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196588c9-715e-4c31-9a77-a2fd69383322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2d7032-8868-41db-85e6-6eab92b8ff15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore our json raw data:\n",
    "display(spark.read.text(volume_folder + \"/user_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc34d73-e995-440e-a65d-b27bae112b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Auto Loader basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf104dd-2ca2-43df-bbbd-983942bd6131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = (spark.readStream\n",
    "             .format(\"cloudFiles\")\n",
    "             .option(\"cloudFiles.format\", \"json\")\n",
    "             .option(\"cloudFiles.maxFilesPerTrigger\", \"1\")\n",
    "             .schema(\"address STRING, creation_date STRING, firstname STRING, lastname STRING, id BIGINT\")\n",
    "             .load(volume_folder + \"/user_json\"))\n",
    "\n",
    "display(bronze_df, get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e43a33-9db4-4de7-98d0-a8386865e406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema inference:\n",
    "Specifying the schema manually can be a challenge, especially with dynamic JSON.\n",
    "\n",
    "* Schema inference has always been expensive and slow at scale, but not with Auto Loader. Auto Loader efficiently samples data to infer the schema and stores it under `cloudFiles.schemaLocation` in your bucket. \n",
    "* Additionally, `cloudFiles.inferColumnTypes` will determine the proper data type from your JSON.\n",
    "\n",
    "*Notes:*\n",
    "* *With Delta Live Tables you don't even have to set this option, the engine manages the schema location for you.*\n",
    "* *Sampling size can be changed with `spark.databricks.cloudFiles.schemaInference.sampleSize.numBytes`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b9f082-b43f-489d-84cc-c862a8f7e3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = (spark.readStream\n",
    "             .format(\"cloudFiles\")\n",
    "             .option(\"cloudFiles.format\", \"json\")\n",
    "             .option(\"cloudFiles.schemaLocation\", volume_folder + \"/inferred_schema\")\n",
    "             .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "             .load(volume_folder + \"/user_json\"))\n",
    "\n",
    "display(bronze_df, get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90185829-5ec9-4739-b4f5-e438aaaecfcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema hints:\n",
    "You might need to enforce a part of your schema, e.g., to convert a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08cd761-ba3f-4969-aad7-c0d49124c80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = (spark.readStream\n",
    "             .format(\"cloudFiles\")\n",
    "             .option(\"cloudFiles.format\", \"json\")\n",
    "             .option(\"cloudFiles.schemaLocation\", volume_folder + \"/inferred_schema\")\n",
    "             .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "             .option(\"cloudFiles.schemaHints\", \"id BIGINT\")\n",
    "             .load(volume_folder + \"/user_json\"))\n",
    "\n",
    "display(bronze_df, get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4887d8a0-3401-4676-838f-d9000b811c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Incorrect schema:\n",
    "Auto Loader automatically recovers from incorrect schema and conflicting type. It'll save incorrect data in the `_rescued_data` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1778cfd7-38c9-4130-9aba-e3fa890466b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding an incorrect field (\"id\" as string instead of bigint):\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(email=\"quentin.ambard@databricks.com\", firstname=\"Quentin\", id=\"456455\", lastname=\"Ambard\")]\n",
    "incorrect_data = spark.createDataFrame(data)\n",
    "\n",
    "(incorrect_data.write\n",
    " .format(\"json\")\n",
    " .mode(\"append\")\n",
    " .save(volume_folder + \"/user_json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fcd8bf-f98d-40ae-b3a9-6254eb21eae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_stream():\n",
    "  return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{volume_folder}/inferred_schema\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"cloudFiles.schemaHints\", \"id BIGINT\")\n",
    "                .load(volume_folder + \"/user_json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2759a869-bb63-4f29-bd2f-929058b7929c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_for_rescued_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1f7421-82e8-41a2-8e52-4430a6f42d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start the stream and filter on on the rescue column to see how the incorrect data is captured\n",
    "display(get_stream().filter(\"_rescued_data IS NOT NULL\"), get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52bc5ae-e464-43a4-9763-8d8b8d4e605a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding a new column\n",
    "By default the stream will tigger a `UnknownFieldException` exception on new column. You then have to restart the stream to include the new column.\n",
    "\n",
    "*Notes*:\n",
    "* *See `cloudFiles.schemaEvolutionMode` for different behaviors and more details.*\n",
    "* *Don't forget to add `.writeStream.option(\"mergeSchema\", \"true\")` to dynamically add columns when writting to a delta table.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6677935b-068e-4892-9bf1-f2e0ea0d101c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Existing stream wil fail with: org.apache.spark.sql.catalyst.util.UnknownFieldException.\n",
    "display(get_stream(), get_chkp_folder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38acf9fb-3224-447f-b30c-e497a1f3f6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add 'new_column':\n",
    "from pyspark.sql import Row\n",
    "data = [Row(email=\"quentin.ambard@databricks.com\", firstname=\"Quentin\", id=456454, lastname=\"Ambard\", new_column=\"test new column value\")]\n",
    "new_row = spark.createDataFrame(data)\n",
    "\n",
    "(new_row.write\n",
    " .format(\"json\")\n",
    " .mode(\"append\")\n",
    " .save(volume_folder + \"/user_json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c60776-46f6-499c-868a-d597bafa787b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We just have to restart it to capture the new data.\n",
    "display(get_stream().filter(\"new_column IS NOT NULL\"), get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c76256c9-fa54-4480-9652-8643ced79675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting a high volume of input files:\n",
    "Scanning folders with many files to detect new data is an expensive operation, leading to ingestion challenges and higher cloud storage costs.\n",
    "\n",
    "To solve this issue and support an efficient listing, Databricks autoloader offers two modes:\n",
    "\n",
    "- Incremental listing with `cloudFiles.useIncrementalListing` (recommended), based on the alphabetical order of the file's path to only scan new data: (`ingestion_path/YYYY-MM-DD`)\n",
    "- Notification system with `cloudFiles.useNotifications`, which sets up a managed cloud notification system sending new file name to a queue.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader-mode.png\" width=\"700\"/>\n",
    "\n",
    "Use the incremental listing option whenever possible. Databricks Auto Loader will try to auto-detect and use the incremental approach when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c99fb7e-0b93-4b5e-9a29-9ed1ee6939e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Support for images:\n",
    "Databricks Auto Loader provides native support for images and binary files.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader-images.png\" width=\"800\" />\n",
    "\n",
    "Just set the format accordingly and the engine will do the rest: `.option(\"cloudFiles.format\", \"binaryFile\")`\n",
    "\n",
    "Use-cases:\n",
    "\n",
    "- ETL images into a Delta table using Auto Loader.\n",
    "- Automatically ingest continuously arriving new images.\n",
    "- Easily retrain ML models on new images.\n",
    "- Perform distributed inference using a pandas UDF directly from Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57886f2f-a066-4092-9ee8-215a4075aa7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploying robust ingestion jobs in production\n",
    "\n",
    "Let's see how to use Auto Loader to ingest JSON files, support schema evolution, and automatically restart when a new column is found.\n",
    "\n",
    "If you need your job to be resilient with regard to an evolving schema, you have multiple options:\n",
    "\n",
    "* Let the full job fail & configure Databricks Workflow to restart it automatically.\n",
    "* Leverage Delta Live Tables to simplify all the setup (DLT handles everything for you out of the box).\n",
    "* Wrap your call to restart the stream when the new column appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e887bbe8-cdbf-4eaf-8579-d55f6aede490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def start_stream_restart_on_schema_evolution():\n",
    "    while True:\n",
    "        try:\n",
    "            stream = (spark.readStream\n",
    "                            .format(\"cloudFiles\")\n",
    "                            .option(\"cloudFiles.format\", \"json\")\n",
    "                            .option(\"cloudFiles.schemaLocation\", volume_folder + \"/inferred_schema\")\n",
    "                            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                            .load(volume_folder + \"/user_json\")\n",
    "                           .writeStream\n",
    "                            .format(\"delta\")\n",
    "                            .option(\"checkpointLocation\", volume_folder + \"/checkpoint\")\n",
    "                            .option(\"mergeSchema\", \"true\")\n",
    "                            .table(\"autoloader_demo_output\"))\n",
    "            stream.awaitTermination()\n",
    "        except BaseException as e:\n",
    "            if not ('UnknownFieldException' in str(e)):\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8de474-30eb-43c2-8429-7f16b1830ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Auto-loader-schema-evolution-Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
