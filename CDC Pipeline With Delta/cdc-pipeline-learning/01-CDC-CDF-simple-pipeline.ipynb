{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "462d4e2b-831f-4ea9-a4ad-dc5e62da3fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implement CDC: Change Data Capture\n",
    "## Use-case: Synchronize your SQL Database with your Lakehouse\n",
    "\n",
    "Delta Lake is an <a href=\"https://delta.io/\" target=\"_blank\">open-source</a> storage layer with Transactional capabilities and increased Performances. \n",
    "\n",
    "Delta lake is designed to support CDC workload by providing support for UPDATE / DELETE and MERGE operation.\n",
    "\n",
    "In addition, Delta table can support CDC to capture internal changes and propagate the changes downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345bef14-9223-4acb-b15b-b43671f4f3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CDC flow\n",
    "\n",
    "Here is the flow we'll implement, consuming CDC data from an external database. \n",
    "\n",
    "Note that the incoming could be any format, including message queue such as Kafka.\n",
    "\n",
    "\n",
    "<img width=\"1000px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/Delta-Lake-CDC-CDF/cdc-flow-0.png\" alt='Make all your data ready for BI and ML'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c08cd5-8481-4f56-a775-95519cd26e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db3a3bda-f71c-4b1f-8294-ba1de492229c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze: Incremental data loading using Auto Loader.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/Delta-Lake-CDC-CDF/cdc-flow-1.png\" alt='Make all your data ready for BI and ML' style='float: right' width='600'/>\n",
    "\n",
    "Working with external system can be challenging due to schema update. The external database can have schema update, adding or modifying columns, and our system must be robust against these changes.\n",
    "\n",
    "Databricks Autoloader (`cloudFiles`) handles schema inference and evolution out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578783b3-be61-4cd8-b2d4-d34ef942818c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Have a look at raw CSV files:\n",
    "cdc_raw_data = (spark.read\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .csv(raw_data_location + \"/user_csv\"))\n",
    "\n",
    "display(cdc_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502e7907-b21f-4e56-9e91-b925a18e96fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Our CDC is sending 3 types of operation: APPEND, DELETE and UPDATE.\n",
    "display(cdc_raw_data.select(\"operation\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b00e22-086a-47c1-bd07-deb4e9312242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV files and write them to delta bronze tables:\n",
    "bronzeDF = (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.maxFilesPerTrigger\", \"1\") # Simulate streaming, remove in production.\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaLocation\", raw_data_location + \"/stream/schema_cdc_raw\")\n",
    "            .option(\"cloudFiles.schemaHints\", \"id BIGINT, operation_date TIMESTAMP\")\n",
    "            .load(raw_data_location + \"/user_csv\")\n",
    "                .withColumn(\"file_name\", F.col(\"_metadata.file_path\")))\n",
    "\n",
    "(bronzeDF.writeStream\n",
    "            .option(\"checkpointLocation\", raw_data_location + \"/stream/checkpoint_cdc_raw\")\n",
    "            .trigger(processingTime = \"10 seconds\")\n",
    "            .table(\"clients_cdc\"))\n",
    "\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f51bdbb-c73e-47a5-a0df-6e86eef656d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED clients_cdc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23caf072-5528-4621-9a70-f1b8369fbb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Let's make sure our table has the proper compaction settings to support streaming:\n",
    "ALTER TABLE clients_cdc\n",
    "  SET TBLPROPERTIES (\n",
    "    delta.autoOptimize.optimizeWrite = true,\n",
    "    delta.autoOptimize.autoCompact = true);\n",
    "\n",
    "SELECT * FROM clients_cdc\n",
    "ORDER BY id ASC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d630731-f62b-4dda-95a5-08823e89dfe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver: Materialize the table.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/Delta-Lake-CDC-CDF/cdc-flow-2.png\" alt='Make all your data ready for BI and ML' style='float: right' width='600'/>\n",
    "\n",
    "The silver `retail_client_silver` table will contains the most up to date view. It'll be a replicate of the original MYSQL table.\n",
    "\n",
    "Because we'll propagate the `MERGE` operations downstream to the `GOLD` layer, we need to enable Delta Lake CDF: `delta.enableChangeDataFeed = true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be7305c-a961-45b8-b165-f68fbefcc627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create our client silver table using standard SQL command:\n",
    "CREATE TABLE IF NOT EXISTS retail_client_silver (\n",
    "    id BIGINT NOT NULL,\n",
    "    name STRING,\n",
    "    address STRING,\n",
    "    email STRING,\n",
    "    operation STRING\n",
    "  )\n",
    "  TBLPROPERTIES (\n",
    "    delta.enableChangeDataFeed = true,\n",
    "    delta.autoOptimize.optimizeWrite = true,\n",
    "    delta.autoOptimize.autoCompact = true\n",
    "  );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5caec9-80d9-4cb3-afbe-2938e66b03e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the MERGE statement to upsert CDC information into our silver table:\n",
    "def merge_stream(df, i):\n",
    "    df.createOrReplaceTempView(\"clients_cdc_microbatch\")\n",
    "\n",
    "    # First, we need to dedup the incoming data based on ID (we can have multiple update of the same row in our incoming data).\n",
    "    #Then we run the merge (upsert or delete).\n",
    "    df.sparkSession.sql(\"\"\"\n",
    "                            MERGE INTO retail_client_silver AS t\n",
    "                                USING (SELECT id, name, address, email, operation\n",
    "                                       FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY id \n",
    "                                                                          ORDER BY operation_date DESC) AS rnk\n",
    "                                             FROM clients_cdc_microbatch)\n",
    "                                       WHERE rnk = 1) AS s\n",
    "                                ON t.id = s.id\n",
    "                                WHEN MATCHED AND s.operation = 'DELETE' THEN DELETE\n",
    "                                WHEN MATCHED AND s.operation != 'DELETE' THEN UPDATE SET *\n",
    "                                WHEN NOT MATCHED AND s.operation != 'DELETE' THEN INSERT *\n",
    "                        \"\"\")\n",
    "\n",
    "(spark.readStream\n",
    "        .table(\"clients_cdc\")\n",
    "      .writeStream\n",
    "        .foreachBatch(merge_stream)\n",
    "        .option(\"checkpointLocation\", raw_data_location + \"/stream/checkpoint_clients_cdc\")\n",
    "        .trigger(processingTime = \"10 seconds\")\n",
    "        .start())\n",
    "\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc425d4-7780-49cd-964b-ec9aefb8c51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM retail_client_silver\n",
    "ORDER BY id ASC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2983c6a4-5983-44c7-bbe7-47e9b2fc040c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing the first CDC layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600da5e6-000f-42cd-b6fa-613d7bbc0cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO clients_cdc (id, name, address, email, operation_date, operation, _rescued_data, file_name) \n",
    "VALUES (1000, \"Quentin\", \"Paris 75020\", \"quentin.ambard@databricks.com\", now(), \"UPDATE\", null, null),\n",
    "       (2000, null, null, null, now(), \"DELETE\", null, null);\n",
    "\n",
    "SELECT * FROM clients_cdc WHERE id IN (1000, 2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7f7541-0209-4777-82af-4c1898a8f4d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM retail_client_silver WHERE id IN (1000, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cbc3ae-18ae-44ce-9e77-644a59457a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold: Capture and Propagate Silver modifications downstream.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/Delta-Lake-CDC-CDF/cdc-flow-3.png\" alt='Make all your data ready for BI and ML' style='float: right' width='600'/>\n",
    "\n",
    "We need to add a final Gold layer based on the data from the Silver table. If a row is DELETED or UPDATED in the SILVER layer, we want to apply the same modification in the GOLD layer.\n",
    "\n",
    "To do so, we need to capture all the tables changes from the SILVER layer and incrementally replicate the changes to the GOLD layer.\n",
    "\n",
    "This is very simple using Delta Lake CDF from our SILVER table!\n",
    "\n",
    "Delta Lake CDF provides the `table_changes('< table_name >', < delta_version >)` that you can use to select all the tables modifications from a specific Delta version to another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63394351-397b-4347-b7c7-1b854feafcf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Delta Lake CDF works using table_changes function:\n",
    "SELECT * FROM table_changes('retail_client_silver', 1)\n",
    "ORDER BY id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a38b29b-d382-43b2-945c-d4e4e2d30c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delta CDF table_changes output\n",
    "Table Changes provides back 4 cdc types in the \"_change_type\" column:\n",
    "\n",
    "| CDC Type             | Description                                                               |\n",
    "|----------------------|---------------------------------------------------------------------------|\n",
    "| **update_preimage**  | Content of the row before an update                                       |\n",
    "| **update_postimage** | Content of the row after the update (what you want to capture downstream) |\n",
    "| **delete**           | Content of a row that has been deleted                                    |\n",
    "| **insert**           | Content of a new row that has been inserted                               |\n",
    "\n",
    "Therefore, 1 update will result in 2 rows in the cdc stream (one row with the previous values, one with the new values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1f08ef-eb5b-46e6-ac8a-a2fe98f71093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "latest_version = str(DeltaTable.forName(spark, \"retail_client_silver\").history(1).head()[\"version\"])\n",
    "\n",
    "print(f\"our Delta table last version is {latest_version}, let's select the last changes to see our DELETE and UPDATE operations (last 2 versions):\")\n",
    "\n",
    "changes = (spark.read\n",
    "                    .format(\"delta\")\n",
    "                    .option(\"readChangeData\", \"true\")\n",
    "                    .option(\"startingVersion\", int(latest_version) - 1)\n",
    "                    .table(\"retail_client_silver\"))\n",
    "\n",
    "display(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb5f65b5-ad52-484c-a4e0-2509f52f4c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Synchronizing our downstream GOLD table based from the Silver changes\n",
    "\n",
    "Let's now say that we want to perform another table enhancement and propagate these changes downstream.\n",
    "\n",
    "To keep this example simple, we'll just add a column name `gold_data` with random data, but in real world this could be an aggregation, a join with another datasource, an ML model etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f14ae65-d8e4-4602-8efd-660453ad7a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS retail_client_gold (\n",
    "  id BIGINT NOT NULL,\n",
    "  name STRING,\n",
    "  address STRING,\n",
    "  email STRING,\n",
    "  gold_data STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0ee547-2fa0-42d3-acef-535afe3c5cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Function to upsert `microBatchOutputDF` into Delta table using MERGE:\n",
    "def upsertToDelta(data, batchId):\n",
    "    # Need to deduplicate based on the id and take the most recent update:\n",
    "    windowSpec = Window.partitionBy(\"id\").orderBy(F.col(\"_commit_version\").desc())\n",
    "    data_deduplicated = (data.withColumn(\"rnk\", F.dense_rank().over(windowSpec))\n",
    "                             .where(\"rnk = 1 and _change_type != 'update_preimage'\")\n",
    "                             .drop(\"_commit_version\", \"rnk\"))\n",
    "    \n",
    "    # Add some data cleaning for the gold layer to remove quotes from the address:\n",
    "    data_deduplicated = data_deduplicated.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \"\\\"\", \"\"))\n",
    "\n",
    "    # Run the merge in the gold table directly:\n",
    "    (DeltaTable.forName(spark, \"retail_client_gold\").alias(\"t\")\n",
    "        .merge(data_deduplicated.alias(\"s\"), \"s.id = t.id\")\n",
    "        .whenMatchedDelete(\"s._change_type = 'delete'\")\n",
    "        .whenMatchedUpdateAll(\"s._change_type != 'delete'\")\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "(spark.readStream\n",
    "        .option(\"readChangeData\", \"true\")\n",
    "        .option(\"startingVersion\", 1)\n",
    "        .table(\"retail_client_silver\")\n",
    "        .withColumn(\"gold_data\", F.lit(\"CDF!\"))\n",
    "      .writeStream\n",
    "        .foreachBatch(upsertToDelta)\n",
    "        .option(\"checkpointLocation\", raw_data_location + \"/stream/checkpoint_clients_gold\")\n",
    "        .trigger(processingTime = \"10 seconds\")\n",
    "        .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495403cd-a83a-4909-9883-a098c83f9d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM retail_client_gold;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b34e5b4-33f9-4725-9222-0240be2401ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6439639798708853,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-CDC-CDF-simple-pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
