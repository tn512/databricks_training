{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8271882-88bc-407b-809a-279b5d065d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Full demo: Change Data Capture on multiple tables\n",
    "## Use-case: Synchronize all your ELT tables with your Lakehouse\n",
    "\n",
    "Real use-case typically includes multiple tables that we need to ingest and synch.\n",
    "\n",
    "These tables are stored on different folder having the following layout:\n",
    "\n",
    "<img width=\"1000px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/Delta-Lake-CDC-CDF/cdc-full.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c9d58c-da30-4062-a99b-bcadcb5dd8d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb12f72-4794-4130-be8f-3e17489b5b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Running the streams in parallel\n",
    "\n",
    "Each table will be save as a distinct table, using a distinct Spark Structured Streaming stream.\n",
    "\n",
    "To implement an efficient pipeline, we should process multiple streams at the same time. To do that, we'll use a ThreadPoolExecutor and start multiple thread, each of them processing and waiting for a stream.\n",
    "\n",
    "We're using Trigger Once to refresh all the tables once and then shutdown the cluster, typically every hour. For lower latencies we can keep the streams running (depending of the number of tables & cluster size), or keep the Trigger Once but loop forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b2c9277-9994-4500-b091-6134af0c6c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema evolution\n",
    "\n",
    "By organizing the raw incoming cdc files with 1 folder by table, we can easily iterate over the folders and pickup any new tables without modification.\n",
    "\n",
    "Schema evolution will be handled my the Autoloader and Delta `mergeSchema` option at the bronze layer. Schema evolution for MERGE (Silver Layer) are supported using `spark.databricks.delta.schema.autoMerge.enabled`\n",
    "\n",
    "*Note: that autoloader will trigger an error in a stream if a schema change happens, and will automatically recover during the next run. See Autoloader demo for a complete example.*\n",
    "\n",
    "*Note: another common pattern is to redirect all the CDC events to a single message queue (the table name being a message attribute), and then dispatch the message in different Silver Tables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd49829-2f63-4501-a9ea-dc1f24e81086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore our raw cdc data:\n",
    "base_folder = f\"{raw_data_location}/cdc\"\n",
    "display(dbutils.fs.ls(f\"{raw_data_location}/cdc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c347984-2313-4122-902f-037b3e7e8a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reset all checkpoints:\n",
    "dbutils.fs.rm(f\"{raw_data_location}/cdc_full\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c59d9d37-4cc2-495e-80e0-8d49c3a15544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze ingestion with autoloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c71a70-643f-4af3-bacf-bad2f06ff673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_bronze_layer(path, bronze_table):\n",
    "    print(f\"Ingesting RAW cdc data for {bronze_table} and building bronze layer...\")\n",
    "\n",
    "    (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{raw_data_location}/cdc_full/schemas/{bronze_table}\")\n",
    "            .option(\"cloudFiles.schemaHints\", \"id BIGINT, operation_date TIMESTAMP\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .load(path)\n",
    "          .withColumn(\"file_name\", F.col(\"_metadata.file_path\"))\n",
    "          .writeStream\n",
    "            .option(\"checkpointLocation\", f\"{raw_data_location}/cdc_full/checkpoints/{bronze_table}\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .trigger(availableNow=True)\n",
    "            .table(bronze_table).awaitTermination())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f18ada-c23c-441e-8162-32a4d40e8a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver materializing tables with MERGE based on CDC events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ab7d86-80b5-4c08-840d-348c515d9515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_silver_layer(bronze_table, silver_table):\n",
    "  print(f\"Ingesting {bronze_table} update and materializing silver layer using a MERGE statement...\")\n",
    "\n",
    "  # First, create the silver table if it doesn't exists:\n",
    "  if not spark.catalog.tableExists(silver_table):\n",
    "    print(f\"Table {silver_table} doesn't exist, creating it using the same schema as the bronze one...\")\n",
    "    (spark.read.table(bronze_table)\n",
    "            .drop(\"operation\", \"operation_date\", \"_rescued_data\", \"file_name\")\n",
    "          .write.saveAsTable(silver_table))\n",
    "  \n",
    "  # For each batch/incremental update from the raw cdc table, we'll run a MERGE on the silver table:\n",
    "  def merge_stream(updates, i):\n",
    "    # Deduplicate based on the id and take the most recent update:\n",
    "    windowSpec = Window.partitionBy(\"id\").orderBy(F.col(\"operation_date\").desc())\n",
    "\n",
    "    updates_deduplicated = (updates.withColumn(\"rnk\", F.row_number().over(windowSpec))\n",
    "                                   .where(\"rnk = 1\")\n",
    "                                   .drop(\"rnk\", \"operation_date\", \"_rescued_data\", \"file_name\"))\n",
    "    \n",
    "    # Remove the \"operation\" field from the column to update in the silver table:\n",
    "    columns_silver = {c: f\"s.{c}\" for c in spark.read.table(silver_table).columns if c != \"operation\"}\n",
    "\n",
    "    # Run the merge in the silver table directly:\n",
    "    (DeltaTable.forName(spark, silver_table).alias(\"t\")\n",
    "        .merge(updates_deduplicated.alias(\"s\"), \"s.id = t.id\")\n",
    "        .whenMatchedDelete(\"s.operation = 'DELETE'\")\n",
    "        .whenMatchedUpdate(\"s.operation != 'DELETE'\", set=columns_silver)\n",
    "        .whenNotMatchedInsert(\"s.operation != 'DELETE'\", values=columns_silver)\n",
    "        .execute())\n",
    "  \n",
    "  (spark.readStream\n",
    "          .table(bronze_table)\n",
    "        .writeStream\n",
    "          .foreachBatch(merge_stream)\n",
    "          .option(\"checkpointLocation\", f\"{raw_data_location}/cdc_full/checkpoints/{silver_table}\")\n",
    "          .trigger(availableNow=True)\n",
    "          .start().awaitTermination())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c00871b5-2b54-4b7a-8250-d1d0425588d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Starting all the streams\n",
    "\n",
    "We can now iterate over the folders to start the bronze & silver streams for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd9d988-9656-4e80-9d5b-fae16198dfb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import deque\n",
    "\n",
    "def refresh_cdc_table(table):\n",
    "    try:\n",
    "        # Update the bronze table:\n",
    "        bronze_table = f\"bronze_{table}\"\n",
    "        update_bronze_layer(f\"{base_folder}/{table}\", bronze_table)\n",
    "\n",
    "        # Update the silver table:\n",
    "        silver_table = f\"silver_{table}\"\n",
    "        update_silver_layer(bronze_table, silver_table)\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't properly process {bronze_table}\")\n",
    "        raise e\n",
    "\n",
    "# Enable Schema evolution during merges (to capture new columns):\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "# Iterate over all the tables folders:\n",
    "tables = [table_path.name[:-1] for table_path in dbutils.fs.ls(base_folder)]\n",
    "\n",
    "# Start 3 CDC flow at the same time in 3 different thread to speed up ingestion:\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    deque(executor.map(refresh_cdc_table, tables))\n",
    "    print(f\"Database refreshed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d88cc02-2818-41aa-a367-964932943f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze_users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83123643-42c9-47de-ba9d-1def0dabb9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM silver_users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb073d5a-91d2-4c65-b5cb-bf13ce48d1ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze_transactions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f093cbfb-cd79-4a19-b6f4-50c31763045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM silver_transactions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1620e83b-33b1-4269-b54f-ea98f595365c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6439639798708884,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-CDC-CDF-full-multi-tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
