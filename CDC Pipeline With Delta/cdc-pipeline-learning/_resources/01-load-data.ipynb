{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec30aa56-f43b-47a6-b52c-4732033e7291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data initialization notebook. \n",
    "Do not run outside of the main notebook. This will automatically be called based on the reste_all widget value to setup the data required for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0e5cfe8-b987-4601-a33c-42bab55edb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42265254-2fa2-4f59-96d7-4f5d3fb18c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"raw_data_location\", \"/demos/retail/delta_cdf\", \"Raw data location (stating dir)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b010ae-257b-43be-b977-85182fee41f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "from collections import OrderedDict \n",
    "import uuid\n",
    "\n",
    "fake = Faker()\n",
    "fake_firstname = F.udf(fake.first_name)\n",
    "fake_lastname = F.udf(fake.last_name)\n",
    "fake_email = F.udf(fake.ascii_company_email)\n",
    "fake_date = F.udf(lambda:fake.date_time_this_month().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "fake_address = F.udf(lambda: fake.address().replace('\\n', ''))\n",
    "fake_id = F.udf(lambda: str(uuid.uuid4()))\n",
    "\n",
    "raw_data_location = dbutils.widgets.get(\"raw_data_location\")\n",
    "\n",
    "#TODO: need to increment ID for each write batch to avoid duplicate. Could get the max reading existing data, zero if none, and add it ti the ID to garantee almost unique ID (doesn't have to be perfect)  \n",
    "def create_dataset(df):\n",
    "  df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "  df = df.withColumn(\"operation_date\", F.current_timestamp())\n",
    "  df = df.withColumn(\"name\", fake_firstname())\n",
    "  df = df.withColumn(\"email\", fake_email())\n",
    "  df = df.withColumn(\"address\", fake_address())\n",
    "  return df\n",
    "#APPEND\n",
    "Faker.seed(0)\n",
    "df = spark.range(0, 10000)\n",
    "df = create_dataset(df)\n",
    "df = df.withColumn(\"operation\", F.lit('APPEND'))\n",
    "df.repartition(5).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/user_csv\")\n",
    "df.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/cdc/users\")\n",
    "\n",
    "#DELETES\n",
    "Faker.seed(0)\n",
    "df = spark.range(0, 400).repartition(1)\n",
    "df = create_dataset(df)\n",
    "df = df.withColumn(\"operation\", F.lit('DELETE'))\n",
    "df.repartition(1).write.mode(\"append\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/user_csv\")\n",
    "df.repartition(1).write.mode(\"append\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/cdc/users\")\n",
    "\n",
    "#UPDATE\n",
    "Faker.seed(2)\n",
    "df = spark.range(0, 400).repartition(1)\n",
    "df = create_dataset(df)\n",
    "df = df.withColumn(\"operation\", F.lit('UPDATE'))\n",
    "df = df.withColumn(\"id\", F.col('id') + 1000)\n",
    "df.repartition(1).write.mode(\"append\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/user_csv\")\n",
    "df.repartition(1).write.mode(\"append\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/cdc/users\")\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  #Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)\n",
    "    \n",
    "    \n",
    "#Transactions\n",
    "Faker.seed(2)\n",
    "df = spark.range(0, 1000).repartition(1)\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df = df.withColumn(\"operation_date\", F.current_timestamp())\n",
    "df = df.withColumn(\"item_count\", (F.rand(10)*3).cast('int')+1)\n",
    "df = df.withColumn(\"amount\", (F.rand(10)*1000).cast('int')+10)\n",
    "df = df.withColumn(\"operation\", F.lit('UPDATE'))\n",
    "df.repartition(1).write.mode(\"append\").option(\"header\", \"true\").format(\"csv\").save(raw_data_location+\"/cdc/transactions\")\n",
    "\n",
    "cleanup_folder(raw_data_location+\"/user_csv\")  \n",
    "cleanup_folder(raw_data_location+\"/cdc/users\")  \n",
    "cleanup_folder(raw_data_location+\"/cdc/transactions\")  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
