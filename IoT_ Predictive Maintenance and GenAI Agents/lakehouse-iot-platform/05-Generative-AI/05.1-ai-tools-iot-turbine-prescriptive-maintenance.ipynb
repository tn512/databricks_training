{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef035dd9-e392-4d36-a9a5-2c067cb3eb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Generative AI with Databricks\n",
    "\n",
    "## From Predictive to Prescriptive Maintenance\n",
    "Manufacturers face labor shortages, supply chain disruptions, and rising costs, making efficient maintenance essential. Despite investments in maintenance programs, many struggle to boost asset productivity due to technician shortages and poor knowledge-sharing systems. This leads to knowledge loss and operational inefficiencies.\n",
    "\n",
    "<div style=\"font-family: 'DM Sans';\">\n",
    "  <div style=\"width: 400px; color: #1b3139; margin-left: 50px; margin-right: 50px; float: left;\">\n",
    "    <div style=\"color: #ff5f46; font-size:50px;\">73%</div>\n",
    "    <div style=\"font-size:25px; margin-top: -20px; line-height: 30px;\">\n",
    "      of manufacturers struggle to recruit maintenance technicians — McKinsey (2023)\n",
    "    </div>\n",
    "    <div style=\"color: #ff5f46; font-size:50px;\">55%</div>\n",
    "    <div style=\"font-size:25px; margin-top: -20px; line-height: 30px;\">\n",
    "      of manufacturers lack formal knowledge-sharing systems — McKinsey (2023)\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Generative AI can transform maintenance by reducing downtime and improving productivity. While predictive maintenance anticipates failures, Generative AI enables prescriptive maintenance. Using historical data, AI systems can identify issues, generate solutions, and assist technicians, allowing junior staff to perform effectively and freeing experts for complex tasks.\n",
    "<br><br>\n",
    "\n",
    "### From Models to Agent Systems\n",
    "Generative AI is moving from standalone models to modular agent systems ([Zaharia et al., 2024](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)). These systems integrate retrievers, models, prompts, and tools to handle complex tasks. Their modular design allows seamless upgrades (e.g., integrating a new LLM) and adaptation to changing needs.\n",
    "\n",
    "<br><br>\n",
    "<img style=\"float: right; margin-top: 10px;\" width=\"700px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/team_flow_liza.png\" />\n",
    "\n",
    "\n",
    "<br>\n",
    "<div style=\"font-size: 19px; margin-left: 0px; clear: left; padding-top: 10px; \">\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/liza.png\" style=\"float: left;\" width=\"80px\"> \n",
    "<h3 style=\"padding: 10px 0px 0px 5px;\">Liza, a Generative AI engineer, uses the Databricks Intelligence Platform to:</h3>\n",
    "<ul style=\"list-style: none; padding: 0; margin-left: 05%;\">\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">1</div>\n",
    "    Build real-time data pipelines\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">2</div>\n",
    "    Retrieve vectors & features\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">3</div>\n",
    "    Create AI agent tools\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">4</div>\n",
    "    Build & deploy agents\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">5</div>\n",
    "    Operate in batch or real-time\n",
    "  </li>\n",
    "  <li style=\"display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">6</div>\n",
    "    Evaluate agent performance\n",
    "  </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**Databricks empowers Liza with a Data + AI platform for Prescriptive Maintenance.** Let’s explore how to deploy this in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd8987f-0434-40c0-972b-8c1b44f5da95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building Agent Systems with Databricks Mosaic AI agent framework\n",
    "\n",
    "We will build an Agent System designed to generate prescriptive work orders for wind turbine maintenance technicians. This system integrates multiple interacting components to ensure proactive and efficient maintenance, thereby optimizing the overall equipment effectiveness.\n",
    "\n",
    "Databricks simplifies this by providing a built-in service to:\n",
    "\n",
    "- Create and store your AI tools leveraging UC functions\n",
    "- Execute the AI tools in a safe way\n",
    "- Use agents to reason about the tools you selected and chain them together to properly answer your question. \n",
    "\n",
    "At a high level, here is the agent system we will implement in this demo:\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/agent_graph_7.png\" style=\"margin-left: 05%\"  width=\"1000px;\">\n",
    "\n",
    "This notebook creates the three Mosaic AI tools and associated Mosaic AI endpoints, which will be composed together into a agent in notebook [05.2-agent-framework-iot-turbine-prescriptive-maintenance]($./05.2-agent-framework-iot-turbine-prescriptive-maintenance).\n",
    "1. **Turbine predictor** which uses a Model Serving endpoint to predict turbines at risk of failure.\n",
    "2. **Turbine maintenance report retriever**  which uses a Vector Search endpoint to retrieve historical maintenance reports with similar sensor readings.\n",
    "3. **Turbine specifications retriever** which uses a Feature Serving endpoint to retrueve turbine specifications based on id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750a1915-8828-46a5-b3b2-e3dc9108e3a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required external libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.20.2 databricks-vectorsearch==0.49 databricks-feature-engineering==0.8.0 databricks-sdk==0.40.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7998e595-c648-4fdf-9014-2d989b6cf39c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing the Application"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa6b991-48b1-4ea1-a077-6313e625e02f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Create the Turbine Predictor as a tool to predict turbine failure\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/agent_graph_2.png\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "To enable our Agent System to predict turbine failtures based on industrial IoT sensor readings, we will rely on the model we deployed previously in the  [04.1-automl-iot-turbine-predictive-maintenance]($./04.3-running-inference-iot-turbine) notebook. \n",
    "\n",
    "**Make sure you run this notebook to create the model serving endpoint!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60699992-39b8-4778-b2de-c98dd9af11aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using the Model Serving as tool to predict faulty turbines\n",
    "Let's define the turbine predictor tool function our LLM agent will be able to execute. \n",
    "\n",
    "AI agents use [AI Agent Tools](https://docs.databricks.com/en/generative-ai/create-log-agent.html#create-ai-agent-tools) to perform actions besides language generation, for example to retrieve structured or unstructured data, execute code, or talk to remote services (e.g. send an email or Slack message). \n",
    "\n",
    "These functions can contain any logic, from simple SQL to advanced python. Below we wrap the model serving endpoint in a SQL function using '[ai_query function](https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af07461d-4c0e-43c3-ab50-45e155a89783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP FUNCTION IF EXISTS turbine_maintenance_predictor;\n",
    "\n",
    "CREATE OR REPLACE FUNCTION turbine_maintenance_predictor(hourly_timestamp TIMESTAMP, avg_energy DOUBLE, std_sensor_A DOUBLE, std_sensor_B DOUBLE, std_sensor_C DOUBLE, std_sensor_D DOUBLE, std_sensor_E DOUBLE, std_sensor_F DOUBLE, location STRING, model STRING, state STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "COMMENT 'This tool predicts whether or not a turbine is faulty to facilitate proactive maintenance'\n",
    "RETURN\n",
    "(\n",
    "    SELECT ai_query(\n",
    "        'dbdemos_iot_turbine_prediction_endpoint', \n",
    "        named_struct(\n",
    "            'hourly_timestamp', hourly_timestamp,\n",
    "            'avg_energy', avg_energy,\n",
    "            'std_sensor_A', std_sensor_A,\n",
    "            'std_sensor_B', std_sensor_B,\n",
    "            'std_sensor_C', std_sensor_C,\n",
    "            'std_sensor_D', std_sensor_D,\n",
    "            'std_sensor_E', std_sensor_E,\n",
    "            'std_sensor_F', std_sensor_F,\n",
    "            'location', location,\n",
    "            'model', model,\n",
    "            'state', state        ),\n",
    "        'STRING'\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b8f45b-a193-4f8c-ae54-ef8282cd2046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can test out our function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebc93074-70ce-4657-8dce-ca44b0985bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT turbine_maintenance_predictor(\n",
    "    TIMESTAMP '2025-01-14T16:00:00.000+00:00',    -- hourly_timestamp\n",
    "    0.9000803742589635,                           -- avg_energy\n",
    "    2.2081154200781867,                           -- std_sensor_A\n",
    "    2.6012126574143823,                           -- std_sensor_B\n",
    "    2.1075958066966423,                           -- std_sensor_C\n",
    "    2.2081154200781867,                           -- std_sensor_D\n",
    "    2.6012126574143823,                           -- std_sensor_E\n",
    "    2.1075958066966423,                           -- std_sensor_F\n",
    "    'Lexington',                                  -- location\n",
    "    'EpicWind',                                   -- model\n",
    "    'America/New_York'                           -- state\n",
    ") AS prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3c96999-7776-484d-8e6c-5ea468ce4519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Create the Maintenance Report Retriever as a tool to retrieve maintenance reports\n",
    "\n",
    "To enable our Agent System to retrieve relevant historical maintenance reports for turbines predicted to be at risk of failure, we have to index historical maintenance reports into a Vector Search Index. Since our `Turbine Predictor as a tool` requires sensor readings as input, we will use an embedding vector of the sensor readings to retrieve maintenance reports with similar sensor readings. \n",
    "\n",
    "<img src=\"https://github.com/Datastohne/demo/blob/main/Screenshot%202024-06-01%20at%2012.58.23.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "\n",
    "Databricks provides multiple types of vector search indexes:\n",
    "\n",
    "- **Managed embeddings**: you provide a text column and endpoint name and Databricks synchronizes the index with your Delta table. \n",
    "- **Self Managed embeddings**: you compute the embeddings and save them as a field of your Delta Table, Databricks will then synchronize the index.\n",
    "- **Direct index**: when you want to use and update the index without having a Delta Table. **(what we'll use in this demo)**\n",
    "\n",
    "Since we are working with numerical sensor readings, which are non-text inputs, we have to use Vector Search Direct Index. In this section we:\n",
    "1. Create a `Vector Search endpoint`\n",
    "2. Create a `Vector Search Direct Index` \n",
    "3. Create a `Vector Search as tool` \n",
    "4. Query a `Vector Search as tool` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5d6764-dcc2-4488-af06-33cb79a63692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's start by loading the `turbine training dataset`, which we will use to index the historical maintenance reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76753bef-4c20-4f75-aeea-5a919f7d5da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read our dataset into a DataFrame\n",
    "(spark.read.table(\"turbine_training_dataset\").dropDuplicates([\"turbine_id\"])\n",
    "  .select(\"composite_key\", \"sensor_vector\", \"maintenance_report\", \"abnormal_sensor\").filter(\"maintenance_report is not null\")\n",
    "  .write.mode('overwrite').saveAsTable(\"turbine_sensor_reports\"))\n",
    "spark.sql('ALTER TABLE turbine_sensor_reports SET TBLPROPERTIES (delta.enableChangeDataFeed = true)')\n",
    "\n",
    "display(spark.table(\"turbine_sensor_reports\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27ba01f-ea23-478e-b098-e62ce1a57c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating the Vector Search endpoint\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/agent_graph_4.png\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Direct Vector Access Index supports direct read and write of vectors and metadata. The user is responsible for updating this table using the REST API or the Python SDK. This type of index cannot be created using the UI. You must use the REST API or the SDK.\n",
    "\n",
    "A vector search index uses a **Vector search endpoint** to serve the embeddings (you can think about it as your Vector Search API endpoint).\n",
    "\n",
    "Multiple Indexes can use the same endpoint. \n",
    "\n",
    "Let's now create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81836243-b288-4462-9a59-99a7ea4957ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d657fe4-7e47-4423-9cf9-ab5444eee86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true\" width=\"600px\" style=\"float: right\">\n",
    "\n",
    "You can view your endpoint on the [Vector Search Endpoints UI](#/setting/clusters/vector-search). Click on the endpoint name to see all indexes that are served by the endpoint.\n",
    "\n",
    "\n",
    "### Creating the Vector Search Index\n",
    "\n",
    "All we now have to do is to as Databricks to create the index on top of our table. The Delta Table will automatically be synched with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f531549b-163e-4019-b086-3c428c20d470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.turbine_sensor_reports_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  index = vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    source_table_name=f\"{catalog}.{db}.turbine_sensor_reports\",\n",
    "    index_name=vs_index_fullname,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key='composite_key',\n",
    "    embedding_dimension=6,\n",
    "    embedding_vector_column=\"sensor_vector\"\n",
    "  )\n",
    "else:\n",
    "  print(f\"Grabbing existing index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  index = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ec559a-1fc0-4d38-9f64-7fa4982de643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Maintenance Report Retriever function\n",
    "Below, we utilize the _VECTOR\\_SEARCH_ SQL function from Databricks to easily set up our maintenance reports retriever function. Our agent will utilize this function in the subsequent steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b2bd1b-a185-44a3-b331-2dfad2421ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP FUNCTION IF EXISTS turbine_maintenance_reports_retriever\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION turbine_maintenance_reports_retriever(sensor_reading_array ARRAY<DOUBLE>)\n",
    "RETURNS ARRAY<STRING>\n",
    "LANGUAGE SQL\n",
    "RETURN (\n",
    "  SELECT collect_list(maintenance_report) FROM VECTOR_SEARCH(index => '{catalog}.{schema}.turbine_sensor_reports_vs_index', query_vector => sensor_reading_array, num_results => 3) ) \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a084550-020c-4ba1-9e39-2a611335615d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using the Maintenance Report Retriever as tool to retrieve maintenance reports\n",
    "Now we can now use our function in SQL, and it'll be available to our tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "235b99de-ba34-4441-968c-edd843b63978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT turbine_maintenance_reports_retriever(\n",
    "  ARRAY(0.9901583695625525,2.2170412500371417,3.2607344819672837,2.3033028001321516,2.4663900152731313,4.575124113082638)\n",
    "  ) AS reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d965a47-d41a-4200-91e8-c9992614c21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Create the Turbine Specifications Retriever as a tool to retrieve turbine specifications\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/agent_graph_3.png\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "To enable our Agent System to retrieve turbine specifications for turbines predicted to be faulty, we need to serve the DLT `turbine_current_features` feature table through a feature serving endpoint.\n",
    "\n",
    "Databricks Feature Serving offers a unified interface for serving pre-materialized and on-demand features to models or applications deployed outside Databricks. These endpoints automatically scale to handle real-time traffic, ensuring high availability and low latency.\n",
    "\n",
    "This part illustrates how to:\n",
    "1. Create a `FeatureSpec`. A `FeatureSpec` defines a set of features (prematerialized and on-demand) that are served together. \n",
    "2. Create an `Online Table` from a Delta Table.\n",
    "3. Serve the features. To serve features, you create a Feature Serving endpoint with the `FeatureSpec`.\n",
    "4. Create a `Feature Serving as tool` using UC tool functions.\n",
    "5. Query a `Feature Serving as tool` using SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4533c8cc-47ae-4814-9974-f35dd7464ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  \n",
    "###  This demo requires a secret to work:\n",
    "Your Tool will need a secret to authenticate against the online table we create (see [Documentation](https://docs.databricks.com/en/security/secrets/secrets.html)).  <br/>\n",
    "**Note: if you are using a shared demo workspace and you see that the secret is setup, please don't run these steps and do not override its value**<br/>\n",
    "\n",
    "- You'll need to [setup the Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/install.html) on your laptop or using this cluster terminal: <br/>\n",
    "`pip install databricks-cli` <br/>\n",
    "- Configure the CLI. You'll need your workspace URL and a PAT token from your profile page<br>\n",
    "`databricks configure`\n",
    "- Create the dbdemos scope:<br/>\n",
    "`databricks secrets create-scope --scope dbdemos`\n",
    "- Save your service principal secret. It will be used by the Model Endpoint to autenticate. If this is a demo/test, you can use one of your [PAT token](https://docs.databricks.com/en/dev-tools/auth/pat.html).<br>\n",
    "`databricks secrets put-secret <SCOPE_GOES_HERE> <KEY_GOES_HERE> --string-value \n",
    "<SECRET_GOES_HERE>'`\n",
    "\n",
    "*Note: Make sure your service principal has access to the Vector Search index:*\n",
    "\n",
    "```\n",
    "spark.sql('GRANT USAGE ON CATALOG <catalog> TO `<YOUR_SP>`');\n",
    "spark.sql('GRANT USAGE ON DATABASE <catalog>.<db> TO `<YOUR_SP>`');\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "WorkspaceClient().grants.update(c.SecurableType.TABLE, <index_name>, \n",
    "                                changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"<YOUR_SP>\")])\n",
    "WorkspaceClient().secrets.put_acl(scope=dbdemos, principal=\"<YOUR_SP>\", permission=workspace.AclPermission.READ)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "565a9f04-4849-483d-8751-57237953f303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set up a Feature Table\n",
    "\n",
    "We'll use the table `turbine_current_features` we created in our DLT pipeline as our feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "733c79ca-d805-42a1-a393-2ac1b1c6824e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What's required for our Feature Serving endpoint\n",
    "\n",
    "To deploy a Feature Serving endpoint, you need to create a **FeatureSpec**.\n",
    "\n",
    "FeatureSpecs are stored in and mananged by Unity Catalog and appear in the Catalog Explorer.\n",
    "\n",
    "Tables specified in a FeatureSpec must be published to an **online store or an online table** for Online Serving.\n",
    "\n",
    "This demo shows how to setup a **Feature Spec** with a **Feature Function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c63cf82-7e74-4ffc-adcf-4e4838735ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setting up a Databricks Online Table\n",
    "To access the feature table from Feature Serving, you must create an Online Table from the feature table. You can create an online table from the Catalog Explorer UI, Databricks SDK or Rest API. The steps to use Databricks python SDK are described below. For more details, see the Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html#create)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/feature-store/online-tables#create)). For information about required permissions, see Permissions ([AWS](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html#user-permissions)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/feature-store/online-tables#user-permissions))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f7c280d-eb73-4046-8bf0-bf18e9e5f3b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Databricks Online Table Setup"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import *\n",
    "import mlflow\n",
    "\n",
    "w = WorkspaceClient()\n",
    "online_table_name = f\"{catalog}.{db}.turbine_current_features_online\"\n",
    "\n",
    "spec = OnlineTableSpec(\n",
    "    primary_key_columns=[\"turbine_id\"],\n",
    "    source_table_full_name=f\"{catalog}.{db}.turbine_current_features\",\n",
    "    run_triggered=OnlineTableSpecTriggeredSchedulingPolicy.from_dict({'triggered': 'true'}),\n",
    "    perform_full_copy=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    online_table_pipeline = w.online_tables.create(table=OnlineTable(name=online_table_name, spec=spec))\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        pass\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "pprint(w.online_tables.get(online_table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4d8eb3-8b26-4c58-8c24-195c581bfa05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Catalog Turbine Specifications"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "features = [FeatureLookup(\n",
    "    table_name=f\"{catalog}.{db}.turbine_current_features\",\n",
    "    lookup_key=[\"turbine_id\"]\n",
    "  )]\n",
    "\n",
    "# Create a `FeatureSpec` in Unity Catalog\n",
    "try:\n",
    "  fe.create_feature_spec(name=f\"{catalog}.{db}.turbine_features_spec\", features=features)\n",
    "except Exception as e:\n",
    "  if \"already exists\" in str(e):\n",
    "    print(f\"FeatureSpec {catalog}.{db}.turbine_features_spec already exists. Skipping execution\")\n",
    "  else:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b8d4c4b-94d1-4dde-8b61-e515923b7d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Feature Serving endpoint\n",
    "\n",
    "Let's create Feature Serving endpoint using the Databricks Python SDK: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862556d8-3db1-4908-9f93-d9ff4ec5fb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "try:\n",
    " status = w.serving_endpoints.create_and_wait(\n",
    "   name=FEATURE_SERVING_ENDPOINT_NAME,\n",
    "   config = EndpointCoreConfigInput(\n",
    "     served_entities=[\n",
    "       ServedEntityInput(\n",
    "         entity_name=f\"{catalog}.{db}.turbine_features_spec\",\n",
    "         scale_to_zero_enabled=True,\n",
    "         workload_size=\"Small\"\n",
    "       )\n",
    "     ]\n",
    "   ) \n",
    " )\n",
    "\n",
    "except Exception as e:\n",
    "  if \"already exists\" in str(e):\n",
    "    print(f\"Serving endpoint {FEATURE_SERVING_ENDPOINT_NAME} already exists. Skipping execution\")\n",
    "  else:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62e1f74a-0609-48c3-a83b-2c3c25edcc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can now view the status of the Feature Serving Endpoint in the table on the **Serving endpoints** page. Click **Serving** in the sidebar to display the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2370936-782e-4607-a5a9-c78559c5f3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using the Turbine Specifications Retriever as tool to retrieve turbine specifications\n",
    "Next, we define the turbine specifications retriever tool function our LLM agent will be able to execute. To do so, we will wrap the Feature Serving endpoint in a UC tool function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f01cb650-398e-4644-b73a-ca82ae0f701d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "host = WorkspaceClient().config.host\n",
    "\n",
    "spark.sql(f\"\"\"DROP FUNCTION IF EXISTS {catalog}.{db}.turbine_specifications_retriever_with_secret;\"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog}.{db}.turbine_specifications_retriever_with_secret(\n",
    "  turbine_id STRING, databricks_token STRING)\n",
    "RETURNS STRUCT<turbine_id STRING, hourly_timestamp STRING, avg_energy DOUBLE, std_sensor_A DOUBLE, std_sensor_B DOUBLE, std_sensor_C DOUBLE, std_sensor_D DOUBLE, std_sensor_E DOUBLE, std_sensor_F DOUBLE, country STRING, lat STRING, location STRING, long STRING, model STRING, state STRING>\n",
    "LANGUAGE PYTHON\n",
    "AS\n",
    "$$\n",
    "  try:\n",
    "    import requests\n",
    "    headers = {{\"Authorization\": \"Bearer \" + databricks_token}}\n",
    "    #Call our vector search endpoint via simple SQL statement\n",
    "    response = requests.post(\"{host}/serving-endpoints/dbdemos_iot_turbine_feature_endpoint/invocations\", json = {{\"dataframe_records\": [{{\"turbine_id\": turbine_id}}]}}, headers=headers)\n",
    "    return response.json().get('outputs')[0]\n",
    "  except Exception as e:\n",
    "    raise e\n",
    "$$;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eebe548-8301-4be6-bd5e-5c324ff6bcce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add the wrapper"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP FUNCTION IF EXISTS turbine_specifications_retriever;\n",
    "CREATE OR REPLACE FUNCTION turbine_specifications_retriever (turbine_id STRING)\n",
    "  RETURNS STRUCT<turbine_id STRING, hourly_timestamp STRING, avg_energy DOUBLE, std_sensor_A DOUBLE, std_sensor_B DOUBLE, std_sensor_C DOUBLE, std_sensor_D DOUBLE, std_sensor_E DOUBLE, std_sensor_F DOUBLE, country STRING, lat STRING, location STRING, long STRING, model STRING, state STRING>\n",
    "LANGUAGE SQL\n",
    "COMMENT 'This tool returns turbine specifications based on the turbine_id.'\n",
    "  RETURN SELECT turbine_specifications_retriever_with_secret(turbine_id, secret('dbdemos', 'ai_agent_sp_token'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2493113-8016-431a-86ef-d7daa0c0a222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can query our turbine specifcations retriever tool function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23394d3b-e4fa-47af-b29a-4e94583a6975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT turbine_specifications_retriever('25b2116a-ae6c-ff55-ce0c-3f08e12656f1') AS turbine_specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58497bbe-0250-4b84-ba1f-1582b186f46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploring Mosaic AI Tools in Unity Catalog\n",
    "\n",
    "You can now view the UC function tools in Catalog Explorer. Click **Catalog** in the sidebar. In the Catalog Explorer, navigate to your catalog and schema. The UC function tools appears under **Functions**. \n",
    "\n",
    "<img src=\"https://github.com/Datastohne/demo/blob/main/Screenshot%202024-09-18%20at%2016.24.24.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef8825be-e4d2-43d8-8b58-0e70f84b8b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What’s next:\n",
    "Now that we create the Mosaic AI Tools, We will compose them into an agent system that generates maintenance work orders using the Mosaic AI agent framework.\n",
    "\n",
    "Open the [05.2-agent-framework-iot-turbine-prescriptive-maintenance]($./05.2-build-agent-iot-turbine-prescriptive-maintenance) notebook to create and deploy the system."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05.1-ai-tools-iot-turbine-prescriptive-maintenance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
