{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bde715e-b9a6-46ff-ab29-d204e6bca7bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b8e28c-9672-4c20-9e30-b1b57b479207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c0489bf-d17c-401f-a9dc-002db5585667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00008905-9dc2-4a79-bbec-d619accb0eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.setup_schema(catalog, db, reset_all_data, volume_name)\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/{volume_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb5a7d9-6cee-4a08-a6d2-5c933375de93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "major, minor = sys.version_info[:2]\n",
    "assert (major, minor) >= (3, 11), f\"This demo expect python version 3.11, but found {major}.{minor}. \\nUse DBR15.4 or above. \\nIf you're on serverless compute, open the 'Environment' menu on the right of your notebook, set it to >=2 and apply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f7b88fe-3af0-4671-b066-66c04602e7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, sha1, col, initcap, to_timestamp\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "    \n",
    "\n",
    "folder = volume_folder\n",
    "\n",
    "if reset_all_data or DBDemos.is_any_folder_empty([folder+\"/historical_turbine_status\", folder+\"/parts\", folder+\"/turbine\", folder+\"/incoming_data\"]):\n",
    "  #data generation on another notebook to avoid installing libraries (takes a few seconds to setup pip env)\n",
    "  print(f\"Generating data under {folder} , please wait a few sec...\")\n",
    "  path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "  parent_count = path[path.rfind(\"lakehouse-iot-platform\"):].count('/') - 1\n",
    "  prefix = \"./\" if parent_count == 0 else parent_count*\"../\"\n",
    "  prefix = f'{prefix}_resources/'\n",
    "  dbutils.notebook.run(prefix+\"01-load-data\", 600, {\"reset_all_data\": dbutils.widgets.get(\"reset_all_data\"), \"catalog\": catalog, \"db\": db})\n",
    "else:\n",
    "  print(\"data already existing. Run with reset_all_data=true to force a data cleanup for your local demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3967af9b-407c-4855-8319-ffe23cc5187f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def endpoint_exists(vsc, vs_endpoint_name):\n",
    "  try:\n",
    "    return vs_endpoint_name in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]\n",
    "  except Exception as e:\n",
    "    #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "    if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "      print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists\")\n",
    "      return True\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "  for i in range(180):\n",
    "    try:\n",
    "      endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "    except Exception as e:\n",
    "      #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "      if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "        print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status\")\n",
    "        return\n",
    "      else:\n",
    "        raise e\n",
    "    status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\"state\"].upper()\n",
    "    if \"ONLINE\" in status:\n",
    "      return endpoint\n",
    "    elif \"PROVISIONING\" in status or i <6:\n",
    "      if i % 20 == 0: \n",
    "        print(f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "      raise Exception(f'''Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")''')\n",
    "  raise Exception(f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f162f68-1563-40b8-a9b6-f3a5e6f45cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if 'RESOURCE_DOES_NOT_EXIST' not in str(e):\n",
    "            print(f'Unexpected error describing the index. This could be a permission issue.')\n",
    "            raise e\n",
    "    return False\n",
    "    \n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "  for i in range(180):\n",
    "    idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "    index_status = idx.get('status', idx.get('index_status', {}))\n",
    "    status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()\n",
    "    url = index_status.get('index_url', index_status.get('url', 'UNKNOWN'))\n",
    "    if \"ONLINE\" in status:\n",
    "      return\n",
    "    if \"UNKNOWN\" in status:\n",
    "      print(f\"Can't get the status - will assume index is ready {idx} - url: {url}\")\n",
    "      return\n",
    "    elif \"PROVISIONING\" in status:\n",
    "      if i % 40 == 0: print(f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "        raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}''')\n",
    "  raise Exception(f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c42233-2e1e-4c33-a1fd-11cf4dac1178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "def get_last_model_version(model_full_name):\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    # Use the MlflowClient to get a list of all versions for the registered model in Unity Catalog\n",
    "    all_versions = mlflow_client.search_model_versions(f\"name='{model_full_name}'\")\n",
    "    # Sort the list of versions by version number and get the latest version\n",
    "    latest_version = max([int(v.version) for v in all_versions])\n",
    "    # Use the MlflowClient to get the latest version of the registered model in Unity Catalog\n",
    "    return mlflow_client.get_model_version(model_full_name, str(latest_version)).version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "783ac886-e316-47c9-a98d-5bb67d1bb29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def get_shared_warehouse(name=None):\n",
    "    w = WorkspaceClient()\n",
    "    warehouses = w.warehouses.list()\n",
    "\n",
    "    # Check for warehouse by exact name (if provided)\n",
    "    if name:\n",
    "        for wh in warehouses:\n",
    "            if wh.name == name:\n",
    "                return wh\n",
    "\n",
    "    # Define fallback priorities\n",
    "    fallback_priorities = [\n",
    "        lambda wh: wh.name.lower() == \"serverless starter warehouse\",\n",
    "        lambda wh: wh.name.lower() == \"shared endpoint\",\n",
    "        lambda wh: wh.name.lower() == \"dbdemos-shared-endpoint\",\n",
    "        lambda wh: \"shared\" in wh.name.lower(),\n",
    "        lambda wh: \"dbdemos\" in wh.name.lower(),\n",
    "        lambda wh: wh.num_clusters > 0,\n",
    "    ]\n",
    "\n",
    "    # Try each fallback condition in order\n",
    "    for condition in fallback_priorities:\n",
    "        for wh in warehouses:\n",
    "            if condition(wh):\n",
    "                return wh\n",
    "\n",
    "    # Raise an exception if no warehouse is found\n",
    "    raise Exception(\n",
    "        \"Couldn't find any Warehouse to use. Please create one first or pass \"\n",
    "        \"a specific name as a parameter to the get_shared_warehouse(name='xxx') function.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
