{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff95a94-5243-4811-bd53-def6a79e045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Science with Databricks\n",
    "\n",
    "## ML is key to wind turbine farm optimization\n",
    "\n",
    "The current market makes energy even more strategic than before. Being able to ingest and analyze our Wind turbine state is a first step, but this isn't enough to thrive in a very competitive market.\n",
    "\n",
    "We need to go further to optimize our energy production, reduce maintenance cost and reduce downtime. Modern data company achieve this with AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f962dd-e4d3-4be2-8b93-12464319e123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Predictive maintenance - Single click deployment with AutoML\n",
    "\n",
    "Let's see how we can now leverage the sensor data to build a model predictive maintenance model.\n",
    "\n",
    "Our first step as Data Scientist is to analyze and build the features we'll use to train our model.\n",
    "\n",
    "The sensor table enriched with turbine data has been saved within our Delta Live Table pipeline. All we have to do is read this information, analyze it and start an Auto-ML run.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-ds-flow.png\" width=\"1000px\">\n",
    "\n",
    "*Note: Make sure you switched to the \"Machine Learning\" persona on the top left menu.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238ed245-de54-4c10-98a0-4427d94ea558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk==0.40.0 databricks-feature-engineering==0.8.0 mlflow==2.20.2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef2104-f9aa-4d30-a963-4f9525082d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b04d0a8-facd-46d1-a0cf-d3eeecb9c814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data exploration and analysis\n",
    "\n",
    "Let's review our dataset and start analyze the data we have to predict our churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b68eca-e5bd-4c27-b530-847639a4d427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot(sensor_report):\n",
    "  turbine_id = (spark.table('turbine_training_dataset')\n",
    "                        .where(f\"abnormal_sensor = '{sensor_report}' \")\n",
    "                        .limit(1)\n",
    "                        .collect()[0]['turbine_id'])\n",
    "  \n",
    "  df = (spark.table('sensor_bronze')\n",
    "                .where(f\"turbine_id == '{turbine_id}'\")\n",
    "                .orderBy('timestamp')\n",
    "                .limit(500)\n",
    "                .pandas_api())\n",
    "  \n",
    "  df.plot(x=\"timestamp\", \n",
    "          y=[\"sensor_B\"], \n",
    "          kind=\"line\", \n",
    "          title=f'Sensor report: {sensor_report}').show()\n",
    "\n",
    "plot('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456536f9-019d-4d7b-aa3a-a87c1c5d8670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot('sensor_B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "365700d2-29a3-42b7-97b2-990a3f215075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see in these graph, we can clearly see some anomaly on the readings we get from sensor B. Let's continue our exploration and use the std we computed in our main feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939660af-43f7-4b70-9113-c26ee9d65672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read our churn_features table:\n",
    "turbine_dataset = (spark.table(\"turbine_training_dataset\")\n",
    "                        .withColumn(\"damaged\", F.col(\"abnormal_sensor\") != \"ok\"))\n",
    "\n",
    "display(turbine_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03599c7d-332f-4474-8d0b-c3ac024e4543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Damaged sensors clearly have a different distribution:\n",
    "import seaborn as sns\n",
    "\n",
    "g = sns.PairGrid(turbine_dataset.sample(0.01).toPandas()[['std_sensor_A', 'std_sensor_E', 'damaged', 'avg_energy']],\n",
    "                 diag_sharey=False,\n",
    "                 hue='damaged')\n",
    "\n",
    "(g.map_lower(sns.kdeplot)\n",
    "  .map_diag(sns.kdeplot, lw=3)\n",
    "  .map_upper(sns.regplot)\n",
    "  .add_legend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42116ab4-1090-4018-b92d-5a3e434aa822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Further data analysis and preparation using pandas API\n",
    "\n",
    "Because our Data Scientist team is familiar with Pandas, we'll use `pandas on spark` to scale `pandas` code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.\n",
    "\n",
    "*Note: Starting from `spark 3.2`, koalas is builtin and we can get an Pandas Dataframe using `pandas_api()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5a546b-fa7c-484f-b639-4992bf8bd4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to pandas (koalas):\n",
    "dataset = turbine_dataset.pandas_api()\n",
    "\n",
    "# Select the columns we would like to use as ML Model features. \n",
    "# Note: we removed percentiles_sensor_A/B/C.. feature to make the demo easier:\n",
    "columns = [\"turbine_id\",\n",
    "           \"hourly_timestamp\",\n",
    "           \"avg_energy\",\n",
    "           \"std_sensor_A\",\n",
    "           \"std_sensor_B\",\n",
    "           \"std_sensor_C\",\n",
    "           \"std_sensor_D\",\n",
    "           \"std_sensor_E\",\n",
    "           \"std_sensor_F\",\n",
    "           \"location\",\n",
    "           \"model\",\n",
    "           \"state\",\n",
    "           \"abnormal_sensor\"]\n",
    "\n",
    "dataset = dataset[columns]\n",
    "\n",
    "# Drop missing values:\n",
    "dataset = dataset.dropna()\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "726c73e6-985b-48e8-82df-6956ea29e78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Write to Feature Store (Optional)\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/mlops-end2end-flow-feature-store.png\" style=\"float:right\" width=\"500\" />\n",
    "\n",
    "Once our features are ready, we'll save them in Databricks Feature Store. Under the hood, features store are backed by a Delta Lake table.\n",
    "\n",
    "This will allow discoverability and reusability of our feature accross our organization, increasing team efficiency.\n",
    "\n",
    "Feature store will bring traceability and governance in our deployment, knowing which model is dependent of which set of features. It also simplify realtime serving.\n",
    "\n",
    "Make sure you're using the \"Machine Learning\" menu to have access to your feature store using the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4acf9a-fa2c-46f4-be1b-93ec1f2384ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS turbine_hourly_features\")\n",
    "    fe.drop_table(name=f\"{catalog}.{db}.turbine_hourly_features\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "churn_feature_table = fe.create_table(\n",
    "    name=f\"{catalog}.{db}.turbine_hourly_features\",\n",
    "    primary_keys=[\"turbine_id\", \"hourly_timestamp\"],\n",
    "    schema=dataset.spark.schema(),\n",
    "    description=\"These features are derived from the turbine_training_dataset table in the data intelligence platform.  We made some basic transformations and removed NA value.\"\n",
    ")\n",
    "\n",
    "fe.write_table(df=dataset.drop_duplicates(subset=[\"turbine_id\", \"hourly_timestamp\"]).to_spark(),\n",
    "               name=f\"{catalog}.{db}.turbine_hourly_features\")\n",
    "\n",
    "features = fe.read_table(name=f\"{catalog}.{db}.turbine_hourly_features\")\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcfa0c07-9683-4b03-8cbd-a8648a74c2ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Accelerating Predictive maintenance model creation using MLFlow and Databricks Auto-ML\n",
    "\n",
    "MLflow is an open source project allowing model tracking, packaging and deployment. Everytime your datascientist team work on a model, Databricks will track all the parameter and data used and will save it. This ensure ML traceability and reproductibility, making it easy to know which model was build using which parameters/data.\n",
    "\n",
    "### A glass-box solution that empowers data teams without taking away control\n",
    "\n",
    "While Databricks simplify model deployment and governance (MLOps) with MLFlow, bootstraping new ML projects can still be long and inefficient. \n",
    "\n",
    "Instead of creating the same boilerplate for each new project, Databricks Auto-ML can automatically generate state of the art models for Classifications, regression, and forecast.\n",
    "\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png\"/>\n",
    "\n",
    "\n",
    "Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks of efforts.\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "<img style=\"float: right\" width=\"600\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-auto-ml.png\"/>\n",
    "\n",
    "### Using Databricks Auto ML with our Churn dataset\n",
    "\n",
    "Auto ML is available in the \"Machine Learning\" space. All we have to do is start a new Auto-ML experimentation and select the feature table we just created (`turbine_hourly_features`)\n",
    "\n",
    "Our prediction target is the `abnormal_sensor` column.\n",
    "\n",
    "Click on Start, and Databricks will do the rest.\n",
    "\n",
    "While this is done using the UI, you can also leverage the [python API](https://docs.databricks.com/applications/machine-learning/automl.html#automl-python-api-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df670c5-f911-4661-a175-cf9eecbfca66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "fe = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089e8ec4-5b6b-4761-82c8-3a2dc13f44fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xp_path = \"/Shared/dbdemos/experiments/lakehouse-iot-platform\"\n",
    "xp_name = f\"automl_iot_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "\n",
    "training_dataset = (fe.read_table(name=f\"{catalog}.{db}.turbine_hourly_features\")\n",
    "                        .drop(\"turbine_id\")\n",
    "                        .sample(0.1)) #Reduce the dataset size to speedup the demo\n",
    "\n",
    "try:\n",
    "    from databricks import automl\n",
    "    automl_run = automl.classify(\n",
    "        experiment_name = xp_name,\n",
    "        experiment_dir = xp_path,\n",
    "        dataset = training_dataset,\n",
    "        target_col = \"abnormal_sensor\",\n",
    "        timeout_minutes = 10\n",
    "    )\n",
    "\n",
    "    #Make sure all users can access dbdemos shared experiment\n",
    "    DBDemos.set_experiment_permission(f\"{xp_path}/{xp_name}\")\n",
    "except Exception as e:\n",
    "    if \"cannot import name 'automl'\" in str(e):\n",
    "        DBDemos.create_mockup_automl_run(f\"{xp_path}/{xp_name}\", training_dataset.toPandas())\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b22b4e-969f-4dc3-841a-3e2efb829f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "AutoML saved our best model in the MLFlow registry. Open the experiment from the AutoML run to explore its artifact and analyze the parameters used, including traceability to the notebook used for its creation.\n",
    "\n",
    "If we're ready, we can move this model into Production stage in a click, or using the API. Let' register the model to Unity Catalog and move it to production.\n",
    "\n",
    "You can programatically get the last best run from your automl training:\n",
    "```\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# retrieve best model trial run\n",
    "trial_id = automl_run.best_trial.mlflow_run_id\n",
    "model_uri = \"runs:/{}/model\".format(automl_run.best_trial.mlflow_run_id)\n",
    "#Use Databricks Unity Catalog to save our model\n",
    "latest_model = mlflow.register_model(model_uri, f\"{catalog}.{db}.{model_name}\")\n",
    "# Flag it as Production ready using UC Aliases\n",
    "MlflowClient().set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"prod\", version=latest_model.version)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04.1-automl-iot-turbine-predictive-maintenance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
